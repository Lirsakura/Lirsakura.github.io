{"meta":{"title":"evitcet's blog","subtitle":"","description":"just a bad code","author":"Evitcet","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2021-11-16T16:00:00.000Z","updated":"2021-11-17T07:21:46.238Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"一个不算有趣的人 喜欢读书和旅游"},{"title":"categories","date":"2021-11-17T05:41:31.501Z","updated":"2021-11-17T05:41:31.501Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"archives","date":"2021-11-17T06:19:40.369Z","updated":"2021-11-17T06:19:40.369Z","comments":true,"path":"archives/index.html","permalink":"http://example.com/archives/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-11-17T05:35:55.496Z","updated":"2021-11-17T05:35:55.496Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-11-17T05:35:08.442Z","updated":"2021-11-17T05:35:08.442Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"travel","date":"2021-11-17T06:56:11.623Z","updated":"2021-11-17T06:56:11.623Z","comments":true,"path":"tags/travel/index.html","permalink":"http://example.com/tags/travel/index.html","excerpt":"","text":""}],"posts":[{"title":"N-gram Language Model","slug":"n-gram","date":"2021-11-17T07:59:52.000Z","updated":"2021-11-17T09:52:05.475Z","comments":true,"path":"2021/11/17/n-gram/","link":"","permalink":"http://example.com/2021/11/17/n-gram/","excerpt":"","text":"Introduce 在自然语言处理中，最常见的一个任务就是预测一个句子中的下一个单词，比如： Please turn your homework ... 在homework后可以连接很多单词，比如in、over，但不可能是接dog。为了更好的预测下一个单词，n-gram模型做的事情就是给可能出现在下一个位置的每个单词分配一个概率。对下一个单词出现概率的计算适用于很多场景，如语法纠错和输入法联想。 n-gram就是计算句子和单词序列概率最简单的模型，其中n指的是一个有n个单词序列的句子，如2-gram（‘I like’,‘I want’）或者3-gram（‘please turn on’,‘I like you’）等。 n-gram模型要做的就是根据给定的序列中前n-1个单词来估计第n个单词的概率，并且为整个句子分配一个概率。 假设有一个序列为：S=‘我最喜欢自然语’，S的长度为7，如果我们想要计算下一个单词w=“言”的概率，那么数学表达式为P(w∣S)P(w|S)P(w∣S)，也可以写成P(言∣我最喜欢自然语)P(言|我最喜欢自然语)P(言∣我最喜欢自然语)。 通常我们会使用相对频率计数来估计这种概率，即在一个非常大语料库先统计S（“我最喜欢自然语”）出现的次数，接着统计S+w（“我最喜欢自然语言”）出现的次数，通过如下公式即可得出P(w∣S)P(w|S)P(w∣S)结果。 P(w∣S)=Count(S+w)Count(S)P(w|S)=\\frac{Count(S+w)}{Count(S)} P(w∣S)=Count(S)Count(S+w)​ 如果想要计算序列的联合概率，即P(S)P(S)P(S)，那么就需要知道S在语料库所有长度为7的序列中的占比。但统计包含百万条数据的语料库中长度为7序列出现的次数非常困难。 Method 链式法则 概率的链式法则如下，其中X1:n−1=X1,X2,...,XnX_{1:n-1}=X_1,X_2,...,X_nX1:n−1​=X1​,X2​,...,Xn​ P(X1,...,Xn)=P(X1)P(X2∣X1)P(X3∣X1:2)...P(Xn∣X1:n−1)=∏k=1nP(Xk∣X1:k−1)\\begin{aligned} P(X_1,...,X_n)&amp;=P(X_1)P(X_2|X_1)P(X_3|X_{1:2})...P(X_n|X_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(X_k|X_{1:k-1}) \\end{aligned}P(X1​,...,Xn​)​=P(X1​)P(X2​∣X1​)P(X3​∣X1:2​)...P(Xn​∣X1:n−1​)=k=1∏n​P(Xk​∣X1:k−1​)​ 根据上述我们已知，假设存在长度为n的序列S，计算P(w1,w2,...,wn)P(w_1,w_2,...,w_n)P(w1​,w2​,...,wn​)，可以转化为如下算式 P(w1:n)=P(w1)P(w2∣w1)P(w3∣w1:2)...P(wn∣w1:n−1)=∏k=1nP(wk∣w1:k−1)\\begin{aligned} P(w_{1:n})&amp;=P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(w_k|w_{1:k-1}) \\end{aligned}P(w1:n​)​=P(w1​)P(w2​∣w1​)P(w3​∣w1:2​)...P(wn​∣w1:n−1​)=k=1∏n​P(wk​∣w1:k−1​)​ 但链式法则的计算也非常冗杂，为了减小计算量我们可以通过使用马尔科夫链来近似估计P(w1:n)P(w_{1:n})P(w1:n​) 马尔科夫链 Reference N-gram Language Model","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://example.com/tags/model/"}]},{"title":"A Neural Probabilistic Language Model","slug":"NNML","date":"2021-11-17T07:42:48.000Z","updated":"2021-11-17T08:08:09.221Z","comments":true,"path":"2021/11/17/NNML/","link":"","permalink":"http://example.com/2021/11/17/NNML/","excerpt":"","text":"论文阅读摘要部分：首先介绍了统计语言建模的一个难题，测试模型的单词序列可能与训练中看到的所有单词序列都不同，会导致维度诅咒（curse of dimensionality）问题的发生。","categories":[{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"}],"tags":[{"name":"nlp,paper","slug":"nlp-paper","permalink":"http://example.com/tags/nlp-paper/"}]},{"title":"nltk使用指南","slug":"nltk使用指南","date":"2021-11-16T12:28:53.000Z","updated":"2021-11-17T05:43:05.120Z","comments":true,"path":"2021/11/16/nltk使用指南/","link":"","permalink":"http://example.com/2021/11/16/nltk%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"安装方式先使用pip安装pip install nltk，接着在命令行中输入如下句子，即可下载。 12import nltknltk.download() 如果报错，则修改Sever Index为http://www.nltk.org/nltk_data/即可下载任意数据。但也可能存在即使改了也不生效的情况，可以上网查找一波前辈们已经整理好的文件夹，按照他们的博文里面的方式解压存放。 这里注意解压之后把package改名为nltk_data后移动到相应的位置即可。 使用方法查找指定单词上下文，使用函数concorance 1text1.concordance(&#x27;monstrous&#x27;) 查找文本中意思相近的词语，使用函数similar 1text1.similar(&#x27;monstrous&#x27;) 查找两个及以上词汇共用的上下文，使用函数common_context 1text2.common_contexts([&#x27;monstrous&#x27;,&#x27;very&#x27;]) 根据某一风格生成随机文本，使用函数generate 1text2.generate() 查看文本中某一词汇出现的次数 1text3.count(&#x27;smote&#x27;) 可以通过如下函数实现文本词汇丰富度测量 12def lexical_diversity(text): return len(text)/len(set(text)) 查看单个词汇在全文中占比 123def percentage(count,total): return 100*count/totalpercentage(text4.count(&#x27;a&#x27;),len(text4)) 查找给定单词在文本中的频率分布，使用FreqDist，该函数的返回值为一个字典。 12345fdist1=FreqDist(text1)fdist1#outputFreqDist(&#123;&#x27;,&#x27;: 18713, &#x27;the&#x27;: 13721, &#x27;.&#x27;: 6862, &#x27;of&#x27;: 6536, &#x27;and&#x27;: 6024, &#x27;a&#x27;: 4569, &#x27;to&#x27;: 4542, &#x27;;&#x27;: 4072, &#x27;in&#x27;: 3916, &#x27;that&#x27;: 2982, ...&#125;) 也可以绘制图text1文本最常见的50个词 接着可以查看文本中任意词汇出现的频率 1fdist1.freq(&#x27;very&#x27;) 通过bigrams可以实现词语两两搭配 1234[i for i in bigrams([&#x27;I&#x27;,&#x27;love&#x27;,&#x27;nlp&#x27;])]#output[(&#x27;I&#x27;, &#x27;love&#x27;), (&#x27;love&#x27;, &#x27;nlp&#x27;)] 查找词汇搭配使用collocations() 1text4.collocations()","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"nlp—nltk","slug":"nlp—nltk","permalink":"http://example.com/tags/nlp%E2%80%94nltk/"}]},{"title":"旅游计划","slug":"迪士尼","date":"2021-11-16T08:41:06.000Z","updated":"2021-11-17T07:38:36.064Z","comments":true,"path":"2021/11/16/迪士尼/","link":"","permalink":"http://example.com/2021/11/16/%E8%BF%AA%E5%A3%AB%E5%B0%BC/","excerpt":"","text":"期待迪士尼之行","categories":[{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}],"categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"},{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"},{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://example.com/tags/model/"},{"name":"nlp,paper","slug":"nlp-paper","permalink":"http://example.com/tags/nlp-paper/"},{"name":"nlp—nltk","slug":"nlp—nltk","permalink":"http://example.com/tags/nlp%E2%80%94nltk/"},{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}