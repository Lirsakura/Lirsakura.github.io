{"meta":{"title":"evitcet's blog","subtitle":"","description":"just a bad code","author":"Evitcet","url":"http://example.com","root":"/"},"pages":[{"title":"archives","date":"2021-11-17T06:19:40.369Z","updated":"2021-11-17T06:19:40.369Z","comments":true,"path":"archives/index.html","permalink":"http://example.com/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-11-17T05:41:31.501Z","updated":"2021-11-17T05:41:31.501Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2021-11-16T16:00:00.000Z","updated":"2021-11-17T07:21:46.238Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"一个不算有趣的人 喜欢读书和旅游"},{"title":"friends","date":"2021-11-17T05:35:55.496Z","updated":"2021-11-17T05:35:55.496Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-11-17T05:35:08.442Z","updated":"2021-11-17T05:35:08.442Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"travel","date":"2021-11-17T06:56:11.623Z","updated":"2021-11-17T06:56:11.623Z","comments":true,"path":"tags/travel/index.html","permalink":"http://example.com/tags/travel/index.html","excerpt":"","text":""}],"posts":[{"title":"隐马尔可夫模型","slug":"HHM","date":"2021-11-18T02:14:58.000Z","updated":"2021-11-19T03:45:46.944Z","comments":true,"path":"2021/11/18/HHM/","link":"","permalink":"http://example.com/2021/11/18/HHM/","excerpt":"","text":"0x01 Hidden Markov Model 例子 假设一共有如图所示的3个盒子，分别为A、B和C，每个盒子里一共有两种颜色的球，分别为蓝色和红色。 现在要求小y需要抽球，抽球的方式如下： 以等概率挑选任意1个盒子，再从该盒子里抽取任意1个球，记录颜色之后放回。 接着从当前盒子转移到下一个盒子。（为了简单起见，转移的概率也是相等的13\\frac{1}{3}31​） 从下一个盒子中取出1个球，记录颜色后放回。 小y重复上述抽球行为9次后，得到了一串球颜色的序列，为 O=(r,r,r,b,r,b,r,b,b)O=(r,r,r,b,r,b,r,b,b)O=(r,r,r,b,r,b,r,b,b)。 我们作为这场抽球游戏的观测者，只能观测到小y抽取到的球颜色，并不能观测到小y到底从哪些盒子里取的球。 那么在上述的例子中，存在两个随机序列。在隐马尔科夫模型中，可观测到的序列OOO叫观测（随机）序列，也称为可见状态链；假设选中的盒子序列为III，那么不能被观测到RRR叫做状态（随机）序列，也称为隐含状态链。 假设选中盒子的序列为I={A,A,B,C,B,A,C,A,B}I=\\{A,A,B,C,B,A,C,A,B\\}I={A,A,B,C,B,A,C,A,B}，那么可以结合上述条件绘制出隐马尔科夫模型： 图中第一排表示的是隐含状态链，第二排是可见状态链，绿色箭头表示的是从一个隐含状态到一个可见状态的输出，黄色箭头表示的是从一个隐含状态到下一个隐含状态的输出。 根据条件，我们也可以得到状态集合、观测集合、序列长度以及模型的三要素。 盒子对应着状态，因此状态集合是： Q={A,B,C},N=3Q=\\{A,B,C\\}, N=3 Q={A,B,C},N=3 球的颜色对应着观测，因此观测集合是： V={r,b},M=2V=\\{r,b\\},M=2 V={r,b},M=2 状态序列和观测序列长度均为T=9T=9T=9。 隐马尔科夫模型的模型三要素为初始状态分布πππ，状态转移概率分布AAA和观测概率分布BBB 初始状态分布πππ对应着最初的等概率取盒子： π=(13,13,13)\\pi=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}) π=(31​,31​,31​) 状态转移概率AAA对应着每个盒子转移到下一个盒子的概率分布，本例中均为等概率： [131313131313131313]\\begin{bmatrix} \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3}\\\\ \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3}\\\\ \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} \\end{bmatrix} ⎣⎢⎡​31​31​31​​31​31​31​​31​31​31​​⎦⎥⎤​ 观测概率分布 BBB 对应着每个盒子里各种颜色球的概率分布： [154545152535]\\begin{bmatrix} \\frac{1}{5} &amp; \\frac{4}{5} \\\\ \\frac{4}{5} &amp; \\frac{1}{5} \\\\ \\frac{2}{5} &amp; \\frac{3}{5} \\end{bmatrix} ⎣⎢⎡​51​54​52​​54​51​53​​⎦⎥⎤​ 下面给出HMM的官方定义，如果上述例子已经看懂了，也可以直接跳过定义部分。 定义 隐马尔科夫模型（hidden Markov model，HHM）是一种基于监督学习的模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 定义：马尔科夫链是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测，从而产生观测随机序列的过程。隐马尔科夫链随机生成的状态的序列，成为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。 隐马尔科夫模型是由初始概率分布、状态转移概率分布以及观测概率分布确定，具体形式定义如下： 假设QQQ是所有可能的状态集合，VVV是所有可能的观测集合： Q={q1,q2,...qN},V={v1,v2,...,vM}Q=\\{q_1,q_2,...q_N \\},V=\\{v_1,v_2,...,v_M\\} Q={q1​,q2​,...qN​},V={v1​,v2​,...,vM​} NNN是对应的状态数，MMM是对应的观测数。 假设 III 是长度为 TTT 的状态序列，OOO是对应的观测序列： I=(i1,i2,...,iT),O=(o1,o2,...oT)I=(i_1,i_2,...,i_T),O=(o_1,o_2,...o_T) I=(i1​,i2​,...,iT​),O=(o1​,o2​,...oT​) AAA是状态转移概率矩阵： A=[aij]N×NA=[a_{ij}]_{N\\times N} A=[aij​]N×N​ 其中， aij=P(it+1=qj∣it=qi),i=1,2,...,N;j=1,2,...,Na_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,...,N;j=1,2,...,N aij​=P(it+1​=qj​∣it​=qi​),i=1,2,...,N;j=1,2,...,N aija_{ij}aij​表示在时刻ttt处于状态qiq_iqi​的条件下，在时刻t+1t+1t+1转移到状态qjq_jqj​的概率。 BBB是观测概率矩阵： B=[bj(k)]N×MB=[b_j(k)]_{N\\times M} B=[bj​(k)]N×M​ 其中， bj(k)=P(ot=vk∣it=qj),j=1,2...,M;j=1,2...Nb_j(k)=P(o_t=v_k|i_t=q_j),j=1,2...,M;j=1,2...N bj​(k)=P(ot​=vk​∣it​=qj​),j=1,2...,M;j=1,2...N bj(k)b_j(k)bj​(k)表示处在状态qjq_jqj​条件下生成观测vkv_kvk​的概率， π\\piπ是初始状态概率向量： π=(πi)\\pi=(\\pi_i) π=(πi​) 其中， πi=P(i1=qi),i=1,2,...,N\\pi_i=P(i_1=q_i),i=1,2,...,N πi​=P(i1​=qi​),i=1,2,...,N πi\\pi_iπi​表示时刻t=1t=1t=1处于状态qiq_iqi​的概率。 A,B,πA,B,\\piA,B,π 共同构成了隐马尔科夫模型的三要素，可用三元组表示为λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)。 基本假设 由上述也可以得出，隐马尔科夫模型作了两个基本假设： 齐次马尔科夫假设，即隐藏的马尔科夫链在任意时刻t的状态只依赖前一时刻的状态，与其他时刻的状态以及观测无关，也与时刻t本身无关： P(it∣it−1,ot−1,...,i1,o1)=P(it∣it−1),t=1,2,...,TP(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,T P(it​∣it−1​,ot−1​,...,i1​,o1​)=P(it​∣it−1​),t=1,2,...,T 观测独立性假设，即任意时刻的观测只依赖于该时刻t的马尔科夫链的状态，与其他时刻及状态无关： P(ot∣iT,oT,iT−1,oT−1,...,it+1,ot+1,it,it−1,ot−1,...,i1,o1)=P(it∣it−1),t=1,2,...,T P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,T P(ot​∣iT​,oT​,iT−1​,oT−1​,...,it+1​,ot+1​,it​,it−1​,ot−1​,...,i1​,o1​)=P(it​∣it−1​),t=1,2,...,T 用上述的例子简单解释一下两个基本性质： 齐次马尔科夫假设可以理解为，在抽小球的过程中，决定下一时刻选什么箱子，只和前一时刻状态有关系： 假设在时刻ttt的时候选中的箱子为A，即it=Ai_t=Ait​=A，那么在时刻t+1t+1t+1时选到B箱子的概率为p(it+1=B∣it=A)=13p(i_{t+1}=B|i_{t}=A)=\\frac{1}{3}p(it+1​=B∣it​=A)=31​，选到C箱子的概率为p(it+1=C∣it=A)=13p(i_{t+1}=C|i_{t}=A)=\\frac{1}{3}p(it+1​=C∣it​=A)=31​，选到A箱子的概率为p(it+1=A∣it=A)=13p(i_{t+1}=A|i_{t}=A)=\\frac{1}{3}p(it+1​=A∣it​=A)=31​。转移概率的值仅和上一状态有关，和其他无关。 观测独立性假设可以理解为，在抽小球的过程中，决定当前时刻抽中的球的颜色只和当前时刻的状态有关： 假设在时刻ttt选中的箱子为AAA，也即it=Ai_t=Ait​=A，那么在当前时刻抽中的红球的概率为P(oi=r∣it=A)=15P(o_i=r|i_t=A)=\\frac{1}{5}P(oi​=r∣it​=A)=51​，在当前时刻抽中的蓝球的概率为P(oi=b∣it=A)=45P(o_i=b|i_t=A)=\\frac{4}{5}P(oi​=b∣it​=A)=54​。当前时刻观测概率的值只和当前状态有关，和其他值无关。 0x02 三个问题（三种算法） 在介绍每一种算法之前，我都会给出算法的定义，再用小球问题解释这个算法。 假设盒子和球条件以及规则不变，这次小y取了三次球，获得的观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r) 概率计算算法 在我们已知最终取到的小球观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r)和模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)中的所有值的情况下，那我们可以使用概率计算算法获得在模型λ\\lambdaλ的条件下获得观测序列OOO的概率，即P(O∣λ)P(O|\\lambda)P(O∣λ)。概率计算算法共有三种，分别为： 直接计算法 前向算法 后向算法 直接计算法 算法 这种算法最直接，就是按照概率公式直接计算，先列举出所有可能的长度为TTT的状态序列I=(i1,i2,...,iN)I=(i_1,i_2,...,i_N)I=(i1​,i2​,...,iN​)，接着求出III和QQQ的联合概率P(O,I∣λ)P(O,I|\\lambda)P(O,I∣λ)，最后依次累加所有状态序列下的联合概率求得P(O∣λ)P(O|\\lambda)P(O∣λ)。注：这里加上λ\\lambdaλ只是为了注明模型条件，而不是计算条件概率。 例子 已知观测序列的长度T=3T=3T=3，状态个数N=3N=3N=3，计算可以得知III一共有NT=33=27N^T=3^3=27NT=33=27种排列方式。将每种排列方式下的P(I∣λ)P(I|\\lambda)P(I∣λ)和P(O∣I,λ)P(O|I,\\lambda)P(O∣I,λ)相乘即可求出P(O,I∣λ)P(O,I|\\lambda)P(O,I∣λ)。 这里假设I1=(i1=A,i2=A,i3=A)I_1=(i_1=A,i_2=A,i_3=A)I1​=(i1​=A,i2​=A,i3​=A)，那么: P(I1∣λ)={π,ai1,ai2}={13,13,13}P(I_1|\\lambda)=\\{\\pi,a_{i_1},a_{i_2}\\}=\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\} P(I1​∣λ)={π,ai1​​,ai2​​}={31​,31​,31​} 已知O=(o1=r,o2=b,o3=r)O=(o_1=r,o_2=b,o_3=r)O=(o1​=r,o2​=b,o3​=r)，那么： P(O∣I1λ)={bi1(o1),bi2(o2),bi3(o3)}={P(o1=r∣i1=A),P(o2=b∣i2=A),P(o3=r∣i3=A)}={15,45,15}\\begin{aligned} P(O|I_1\\lambda)&amp;=\\{b_{i_1}(o_1),b_{i_2}(o_2),b_{i_3}(o_3)\\}\\\\&amp;=\\{P(o_1=r|i_1=A),P(o_2=b|i_2=A),P(o_3=r|i_3=A)\\}\\\\&amp;=\\{\\frac{1}{5},\\frac{4}{5},\\frac{1}{5}\\} \\end{aligned}P(O∣I1​λ)​={bi1​​(o1​),bi2​​(o2​),bi3​​(o3​)}={P(o1​=r∣i1​=A),P(o2​=b∣i2​=A),P(o3​=r∣i3​=A)}={51​,54​,51​}​ 则联合概率P(O,I1∣λ)P(O,I_1|\\lambda)P(O,I1​∣λ)为： P(O,I1∣λ)=P(O∣I1,λ)P(I1∣λ)=13∗15∗13∗45∗13∗15=63375 \\begin{aligned} P(O,I_1|\\lambda)&amp;=P(O|I_1,\\lambda)P(I_1|\\lambda)\\\\&amp;=\\frac{1}{3}*\\frac{1}{5}*\\frac{1}{3}*\\frac{4}{5}*\\frac{1}{3}*\\frac{1}{5}\\\\&amp;=\\frac{6}{3375} \\end{aligned}P(O,I1​∣λ)​=P(O∣I1​,λ)P(I1​∣λ)=31​∗51​∗31​∗54​∗31​∗51​=33756​​ 这一步的时间复杂度为O(T)O(T)O(T)。 最后按照上述方式，依次计算出P(O,I2∣λ),...,P(O,I17∣λ)P(O,I_2|\\lambda),...,P(O,I_{17}|\\lambda)P(O,I2​∣λ),...,P(O,I17​∣λ)，累加后即得到P(O∣λ)P(O|\\lambda)P(O∣λ)： P(O∣λ)=∑i=127P(O∣Ii,λ)P(I∣λ)=3923375P(O|\\lambda)=\\sum_{i=1}^{27}P(O|I_i,\\lambda)P(I|\\lambda)=\\frac{392}{3375} P(O∣λ)=i=1∑27​P(O∣Ii​,λ)P(I∣λ)=3375392​ 但上述算法的计算量很大，这里很容易推得，复杂度是O(TNT)O(TN^T)O(TNT)阶。 前向算法 前向算法的定义如下： 给定马尔可夫模型λ\\lambdaλ，定义到时刻ttt部分观测序列为o1,o2,..oto_1,o_2,..o_to1​,o2​,..ot​且状态为qiq_iqi​的概率为前向概率，记作 αt(i)=P(o1,o2,...,ot,it=qi∣λ)\\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\\lambda) αt​(i)=P(o1​,o2​,...,ot​,it​=qi​∣λ) 可以递推地求得前向概率αt(i)\\alpha_t(i)αt​(i)以及观测概率P(O∣λ)P(O|\\lambda)P(O∣λ)。 算法 先计算初值： α1(i)=πibi(o1),i=1,2,...,N\\alpha_1(i)=\\pi_ib_i(o_1), i=1,2,...,N α1​(i)=πi​bi​(o1​),i=1,2,...,N 通过初始化前向概率，得到初始时刻的状态i1=qii_1=q_ii1​=qi​和观测o1o_1o1​的联合概率。 再递推，对t=1,2,...,T−1t=1,2,...,T-1t=1,2,...,T−1： αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2..,N\\alpha_{t+1}(i)=\\left [ \\sum_{j=1}^{N}\\alpha _t(j)a_{ji} \\right ]b_i(o_{t+1}),i=1,2..,N αt+1​(i)=[j=1∑N​αt​(j)aji​]bi​(ot+1​),i=1,2..,N 得出到时刻ttt观测到o1,o2,...,oto_1,o_2,...,o_to1​,o2​,...,ot​并在时刻ttt处于状态qjq_jqj​，而在时刻t+1t+1t+1达到状态qiq_iqi​的联合概率。这里值得注意的是，ajia_{ji}aji​中我们已知的是it+1=qii_{t+1}=q_iit+1​=qi​，未知的部分是it=qji_t=q_jit​=qj​，即由当前状态往前推。 最后得出： P(O∣λ)=∑i=1NαT(i)P(O|\\lambda)=\\sum^N_{i=1}\\alpha_T(i) P(O∣λ)=i=1∑N​αT​(i) 例子 前向算法理解起来不难，但是大部分公式都很繁杂，这里依然用上述的O=(r,b,r)O=(r,b,r)O=(r,b,r)和λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，方便起见使用编号1、2、31、2、31、2、3代替编号A、B、C。 现在我们需要求初值，也即时刻t=1t=1t=1时候的α1(i)\\alpha_1(i)α1​(i)结果。显然，时刻t=1t=1t=1，可能取到的盒子有1、2、31、2、31、2、3三种，这也即α1(1),α1(2),α1(3)\\alpha_1(1),\\alpha_1(2),\\alpha_1(3)α1​(1),α1​(2),α1​(3)。 先假设t=1t=1t=1时取到了111号盒子，即$q_1=1，根据观测序列1，根据观测序列1，根据观测序列o_1=r$，可以得到： α1(1)=P(o1,i1=q1∣λ)=P(o1=r∣i1=q1,λ)P(i1=q1∣λ)=15∗13=115\\begin{aligned} \\alpha_1(1)&amp;=P(o_1,i_1=q_1|\\lambda)\\\\&amp;=P(o_1=r|i_1=q_1,\\lambda)P(i_1=q_1|\\lambda)\\\\&amp;=\\frac{1}{5}*\\frac{1}{3}=\\frac{1}{15} \\end{aligned}α1​(1)​=P(o1​,i1​=q1​∣λ)=P(o1​=r∣i1​=q1​,λ)P(i1​=q1​∣λ)=51​∗31​=151​​ 同理可得α1(2)=415,α1(3)=215\\alpha_1(2)=\\frac{4}{15},\\alpha_1(3)=\\frac{2}{15}α1​(2)=154​,α1​(3)=152​。 在时刻t=2t=2t=2，可能取到的盒子的联合概率，即α2(1),α2(2),α2(3)\\alpha_2(1),\\alpha_2(2),\\alpha_2(3)α2​(1),α2​(2),α2​(3)。 假设t=2t=2t=2取到了222号盒子，即q2=2q_2=2q2​=2，根据已知观测序列o1=r,o2=bo_1=r,o_2=bo1​=r,o2​=b，可以得到： α2(2)=P(o1=r,o2=b,i2=q2∣λ)=[α1(1)a12+α1(2)a22+α1(3)a32]∗P(o2=b∣i2=q2,λ)=[115∗13+415∗13+215∗13]∗15=7225\\begin{aligned} \\alpha_2(2)&amp;=P(o_1=r,o_2=b,i_2=q_2|\\lambda)\\\\&amp;=[\\alpha _1(1)a_{12}+\\alpha _1(2)a_{22}+\\alpha _1(3)a_{32}]*P(o_2=b|i_2=q_2,\\lambda)\\\\&amp;=[\\frac{1}{15}*\\frac{1}{3}+\\frac{4}{15}*\\frac{1}{3}+\\frac{2}{15}*\\frac{1}{3}]*\\frac{1}{5}=\\frac{7}{225} \\end{aligned}α2​(2)​=P(o1​=r,o2​=b,i2​=q2​∣λ)=[α1​(1)a12​+α1​(2)a22​+α1​(3)a32​]∗P(o2​=b∣i2​=q2​,λ)=[151​∗31​+154​∗31​+152​∗31​]∗51​=2257​​ 同理可得α2(1)=28225,α2(3)=21225\\alpha_2(1)=\\frac{28}{225},\\alpha_2(3)=\\frac{21}{225}α2​(1)=22528​,α2​(3)=22521​ 同理计算出α3(1)=563375,α3(3)=2243375,α3(3)=1123375\\alpha_3(1)=\\frac{56}{3375},\\alpha_3(3)=\\frac{224}{3375},\\alpha_3(3)=\\frac{112}{3375}α3​(1)=337556​,α3​(3)=3375224​,α3​(3)=3375112​，累加后得到P(O∣λ)=3923375P(O|\\lambda)=\\frac{392}{3375}P(O∣λ)=3375392​。 综上，前向算法的时间复杂度为O(N2T)O(N^2T)O(N2T)。 后向算法 后向算法和前向算法基本一致，定义如下： 给定马尔可夫模型λ\\lambdaλ，定义在时刻ttt状态为qiq_iqi​的条件下，从t+1t+1t+1到TTT的部分观测序列为ot+1,ot+1,..oTo_{t+1},o_{t+1},..o_Tot+1​,ot+1​,..oT​的概率为后向概率，记作 βt(i)=P(ot+1,ot+2,...,oT∣it=qi,λ)\\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\\lambda) βt​(i)=P(ot+1​,ot+2​,...,oT​∣it​=qi​,λ) 可以递推地求得后向概率βt(i)\\beta_t(i)βt​(i)以及观测概率P(O∣λ)P(O|\\lambda)P(O∣λ)。 算法 先计算终值： βT(i)=1,i=1,2,...,N\\beta_T(i)=1, i=1,2,...,N βT​(i)=1,i=1,2,...,N 再递推，对t=T−1,T−2,...,1t=T-1,T-2,...,1t=T−1,T−2,...,1： βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2..,N\\beta_{t}(i)=\\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),i=1,2..,N βt​(i)=j=1∑N​aij​bj​(ot+1​)βt+1​(j),i=1,2..,N 其中aija_{ij}aij​表示P(it+1=qj∣it=qi)P(i_{t+1}=q_j|i_t=q_i)P(it+1​=qj​∣it​=qi​) 最后得出： P(O∣λ)=∑i=1Nπibi(o1)β1(i)P(O|\\lambda)=\\sum^N_{i=1}\\pi_ib_i(o_{1})\\beta_1(i) P(O∣λ)=i=1∑N​πi​bi​(o1​)β1​(i) 例子 这里依然用上述的O=(r,b,r)O=(r,b,r)O=(r,b,r)和λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，使用编号1、2、31、2、31、2、3代替编号A、B、C​。 假设T=3T=3T=3时刻取到的盒子为1​号盒，即q3=1q_3=1q3​=1。可得β3(1)=1\\beta_3(1)=1β3​(1)=1，同理β3(2)=1,β3(3)=1\\beta_3(2)=1,\\beta_3(3)=1β3​(2)=1,β3​(3)=1。 假设t=2t=2t=2时刻取到了3号盒，则q2=3q_2=3q2​=3，计算此条件下在观测序列为o3=ro_3=ro3​=r的后向概率β2(3)\\beta_2(3)β2​(3)为： β2(3)=P(o3=r,o2=b∣i2=q2,λ)=β3(1)a31P(o3=r∣i3=q1,λ)+β3(2)a32P(o3=r∣i3=q2,λ)+β3(3)a33P(o3=r∣i3=q3,λ)=15∗13+45∗13+25∗13=715\\begin{aligned} \\beta_2(3)&amp;=P(o_3=r,o_2=b|i_2=q_2,\\lambda)\\\\&amp;=\\beta _3(1)a_{31}P(o_3=r|i_3=q_1,\\lambda)+\\beta _3(2)a_{32}P(o_3=r|i_3=q_2,\\lambda)+\\beta _3(3)a_{33}P(o_3=r|i_3=q_3,\\lambda)\\\\&amp;=\\frac{1}{5}*\\frac{1}{3}+\\frac{4}{5}*\\frac{1}{3}+\\frac{2}{5}*\\frac{1}{3}=\\frac{7}{15} \\end{aligned}β2​(3)​=P(o3​=r,o2​=b∣i2​=q2​,λ)=β3​(1)a31​P(o3​=r∣i3​=q1​,λ)+β3​(2)a32​P(o3​=r∣i3​=q2​,λ)+β3​(3)a33​P(o3​=r∣i3​=q3​,λ)=51​∗31​+54​∗31​+52​∗31​=157​​ 同理可得β2(1)=715,β2(2)=715\\beta_2(1)=\\frac{7}{15},\\beta_2(2)=\\frac{7}{15}β2​(1)=157​,β2​(2)=157​ 同理计算出β1(1)=56225,β1(2)=56225,β1(3)=56225\\beta_1(1)=\\frac{56}{225},\\beta_1(2)=\\frac{56}{225},\\beta_1(3)=\\frac{56}{225}β1​(1)=22556​,β1​(2)=22556​,β1​(3)=22556​，带入公式，最后累加后得到P(O∣λ)=21135P(O|\\lambda)=\\frac{21}{135}P(O∣λ)=13521​。 综上，前向算法的时间复杂度为O(N2T)O(N^2T)O(N2T) 学习算法 在我们已知最终取到的小球观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r)的情况下，估计模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)的参数，使得在该模型下的观测序列概率P(O∣λ)P(O|\\lambda)P(O∣λ)最大。 监督学习和无监督学习方法的最大差别在于是否有观测序列对应的状态序列III。 监督学习方法 假设训练数据中包含SSS个长度相同的观测序列和对应的状态序列{(O1,I1),(O2,I2),...,(OS,IS)}\\{(O_1,I_1),(O_2,I_2),...,(O_S,I_S)\\}{(O1​,I1​),(O2​,I2​),...,(OS​,IS​)}，则可以使用极大似然估计法来估计隐马尔可夫模型的参数。 算法 估计转移概率aija_{ij}aij​ 假设样本中时刻 ttt 处于状态iii且时刻 t+1t+1t+1 处于状态 jjj 的频数为 AijA_{ij}Aij​ ，那么转移状态概率aija_{ij}aij​的估计是 a^ij=Aij∑j=1NAij,i=1,2,..N;j=1,2...,N\\hat{a} _{ij}=\\frac{A_{ij}}{\\sum_{j=1}^{N}A_{ij}} ,i=1,2,..N;j=1,2...,N a^ij​=∑j=1N​Aij​Aij​​,i=1,2,..N;j=1,2...,N 估计观测概率bj(k)b_j(k)bj​(k) 假设样本中状态为 jjj 并观测为 kkk 的频数是 BjkB_{jk}Bjk​ ，那么状态为 jjj 并观测为 kkk 的概率是 bj(k)b_j(k)bj​(k) 的估计是 b^j(k)=Bjk∑k=1MBjk,j=1,2,..M;k=1,2...,M\\hat{b} _{j}(k)=\\frac{B_{jk}}{\\sum_{k=1}^{M}B_{jk}} ,j=1,2,..M;k=1,2...,M b^j​(k)=∑k=1M​Bjk​Bjk​​,j=1,2,..M;k=1,2...,M 估计初始状态概率 πi\\pi_iπi​ πi^\\hat{\\pi_i}πi​^​ 为 SSS 个样本中初始状态为 qiq_iqi​ 的频率。 例子 （其实一开始我也没看懂上面说的是什么，不过还好现在懂了(๑•̀ㅂ•́)و✧，我真的很棒棒！） 这里依然使用小球举例子，我们重新拿了三个盒子，依然标为 A,B,CA,B,CA,B,C ，并且规定小y每次有放回地取4个球，并且只取6次，同时还要求小y在抽球的时候记录每次的观测序列 O=(o1,o2,o3,o4)O=(o_1,o_2,o_3,o_4)O=(o1​,o2​,o3​,o4​) 以及对应的状态序列 I=(i1,i2,i3,i4)I=(i_1,i_2,i_3,i_4)I=(i1​,i2​,i3​,i4​)。 小y抽完球之后，给我们发了一张这样的表： 状态序列 观测序列 I1=(A,B,C,B)I_1=(A,B,C,B)I1​=(A,B,C,B) O1=(r,b,r,b)O_1=(r,b,r,b)O1​=(r,b,r,b) I2=(A,A,B,A)I_2=(A,A,B,A)I2​=(A,A,B,A) O2=(r,r,b,b)O_2=(r,r,b,b)O2​=(r,r,b,b) I3=(C,A,B,C)I_3=(C,A,B,C)I3​=(C,A,B,C) O3=(r,b,b,r)O_3=(r,b,b,r)O3​=(r,b,b,r) I4=(A,C,A,B)I_4=(A,C,A,B)I4​=(A,C,A,B) O4=(b,r,b,b)O_4=(b,r,b,b)O4​=(b,r,b,b) I5=(C,A,C,B)I_5=(C,A,C,B)I5​=(C,A,C,B) O5=(r,b,b,r)O_5=(r,b,b,r)O5​=(r,b,b,r) I6=(B,C,C,A)I_6=(B,C,C,A)I6​=(B,C,C,A) O6=(b,b,r,r)O_6=(b,b,r,r)O6​=(b,b,r,r) 根据这张表，我们就可以利用最大似然估计法估计出这次使用的模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，方便起见我们使用编号 1,2,31,2,31,2,3 来代替盒子编号 A,B,CA,B,CA,B,C ，如 a^12\\hat a_{12}a^12​ 就表示从 AAA 盒转移到 BBB 号盒子的概率。 首先估计 a^11\\hat a_{11}a^11​ ，频数 A11A_{11}A11​ 为当前时刻取 AAA 盒且下一时刻依旧取 AAA 盒的次数，也即满足 a11a_{11}a11​ 条件的个数。上表中仅在 I2I_2I2​ 情况下符合a11a_{11}a11​ 条件，因此得到 A11=1A_{11}=1A11​=1 ，同理可得 A12=4,A13=2A_{12}=4,A_{13}=2A12​=4,A13​=2， 因此我们可以估计出a^11=11+4+2=17\\hat a_{11}=\\frac{1}{1+4+2}=\\frac{1}{7}a^11​=1+4+21​=71​ ，也可计算出a^12=47,a^13=27\\hat a_{12}=\\frac{4}{7},\\hat a_{13}=\\frac{2}{7}a^12​=74​,a^13​=72​。依次类推，我们可以得到A^\\hat AA^： A^=[17472714034472717]\\hat A=\\begin{bmatrix} \\frac{1}{7} &amp; \\frac{4}{7} &amp; \\frac{2}{7} \\\\ \\frac{1}{4} &amp; 0 &amp; \\frac{3}{4} \\\\ \\frac{4}{7} &amp;\\frac{2}{7} &amp;\\frac{1}{7} \\end{bmatrix} A^=⎣⎢⎡​71​41​74​​74​072​​72​43​71​​⎦⎥⎤​ 接着估计 b^1(r)\\hat b_1(r)b^1​(r) ，频数 B1rB_{1r}B1r​ 为取 AAA 盒的同时取到了红色球的次数，也即满足条件 b1(r)b_1(r)b1​(r) 的个数，在上表中容易得到 b1(r)=4b_1(r)=4b1​(r)=4 ，同理可得 b1(b)=5b_1(b)=5b1​(b)=5，因此得到 b^1(r)=44+5=49\\hat b_1(r)=\\frac{4}{4+5}=\\frac{4}{9}b^1​(r)=4+54​=94​。以此类推，我们可以得到 B^\\hat BB^： B^=[495917676828]\\hat B=\\begin{bmatrix} \\frac{4}{9} &amp; \\frac{5}{9} \\\\ \\frac{1}{7} &amp; \\frac{6}{7} \\\\ \\frac{6}{8} &amp;\\frac{2}{8} \\end{bmatrix} B^=⎣⎢⎡​94​71​86​​95​76​82​​⎦⎥⎤​ 最后估计 π^1\\hat \\pi_1π^1​ 即样本数据中初始状态为 AAA 盒的频率，显然π^1=36\\hat \\pi_1=\\frac{3}{6}π^1​=63​ ，同理得π^2=16,π^3=26\\hat \\pi_2=\\frac{1}{6},\\hat \\pi_3=\\frac{2}{6}π^2​=61​,π^3​=62​，因此 π^\\hat \\piπ^为： π^=(36,16,26)\\hat \\pi=(\\frac{3}{6},\\frac{1}{6},\\frac{2}{6}) π^=(63​,61​,62​) 无监督学习方法（EM算法）","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"supervise","slug":"supervise","permalink":"http://example.com/tags/supervise/"},{"name":"mechine learning","slug":"mechine-learning","permalink":"http://example.com/tags/mechine-learning/"}]},{"title":"N-gram Language Model","slug":"n-gram","date":"2021-11-17T07:59:52.000Z","updated":"2021-11-18T01:56:34.774Z","comments":true,"path":"2021/11/17/n-gram/","link":"","permalink":"http://example.com/2021/11/17/n-gram/","excerpt":"","text":"Introduce 在自然语言处理中，最常见的一个任务就是预测一个句子中的下一个单词，比如： Please turn your homework ... 在homework后可以连接很多单词，比如in、over，但不可能是接dog。为了更好的预测下一个单词，n-gram模型做的事情就是给可能出现在下一个位置的每个单词分配一个概率。对下一个单词出现概率的计算适用于很多场景，如语法纠错和输入法联想。 n-gram就是计算句子和单词序列概率最简单的模型，其中n指的是一个有n个单词序列的句子，如2-gram（‘I like’,‘I want’）或者3-gram（‘please turn on’,‘I like you’）等。 n-gram模型要做的就是根据给定的序列中前n-1个单词来估计第n个单词的概率，并且为整个句子分配一个概率。 假设有一个序列为：S=“我最喜欢自然语”，S的长度为7，如果我们想要计算下一个单词w=&quot;言&quot;的概率，那么数学表达式为P(w∣S)P(w|S)P(w∣S)，也可以写成P(言∣我最喜欢自然语)P(言|我最喜欢自然语)P(言∣我最喜欢自然语)。 通常我们会使用相对频率计数来估计这种概率，即在一个非常大语料库先统计S（“我最喜欢自然语”）出现的次数，接着统计S+w（“我最喜欢自然语言”）出现的次数，通过如下公式即可得出P(w∣S)P(w|S)P(w∣S)结果。 P(w∣S)=Count(S+w)Count(S)P(w|S)=\\frac{Count(S+w)}{Count(S)} P(w∣S)=Count(S)Count(S+w)​ 当然，我们也可以计算该序列的联合概率P(S)P(S)P(S)，即统计语料库中的S（“我最喜欢自然语”）在所有长度为7的句子中出现的次数。 P(S)=Count(S)Count(所有长度为7的句子)P(S)=\\frac{Count(S)}{Count(所有长度为7的句子)} P(S)=Count(所有长度为7的句子)Count(S)​ 如果语料库中有百万条句子，那么统计所有长度为7的句子数量会变得很麻烦，因此我们通常采用链式法则来估计P(S)P(S)P(S)。 Method 链式法则 概率的链式法则如下，其中X1:n−1=X1,X2,...,XnX_{1:n-1}=X_1,X_2,...,X_nX1:n−1​=X1​,X2​,...,Xn​ P(X1,...,Xn)=P(X1)P(X2∣X1)P(X3∣X1:2)...P(Xn∣X1:n−1)=∏k=1nP(Xk∣X1:k−1)\\begin{aligned} P(X_1,...,X_n)&amp;=P(X_1)P(X_2|X_1)P(X_3|X_{1:2})...P(X_n|X_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(X_k|X_{1:k-1}) \\end{aligned}P(X1​,...,Xn​)​=P(X1​)P(X2​∣X1​)P(X3​∣X1:2​)...P(Xn​∣X1:n−1​)=k=1∏n​P(Xk​∣X1:k−1​)​ 根据上述我们已知，假设存在长度为n的序列S，计算P(w1,w2,...,wn)P(w_1,w_2,...,w_n)P(w1​,w2​,...,wn​)，可以转化为如下算式 P(w1:n)=P(w1)P(w2∣w1)P(w3∣w1:2)...P(wn∣w1:n−1)=∏k=1nP(wk∣w1:k−1)\\begin{aligned} P(w_{1:n})&amp;=P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(w_k|w_{1:k-1}) \\end{aligned}P(w1:n​)​=P(w1​)P(w2​∣w1​)P(w3​∣w1:2​)...P(wn​∣w1:n−1​)=k=1∏n​P(wk​∣w1:k−1​)​ 如果直接计算上式，我们需要获得n个不同的P(wk∣w1:k−1)P(w_k|w_{1:k-1})P(wk​∣w1:k−1​)，计算起来也很麻烦。因此n-gram模型为了减轻计算量，假设只用少量的已知词来估计下一个词，比如2-gram(bigram)，来解决上述问题。 bigram 假设想要在已知前n-1个次的请况下估计第n个词出现的概率，即计算P(wn∣w1:n−1)P(w_n|w_{1:n-1})P(wn​∣w1:n−1​)。bigram模型通常的做法是，用P(wn∣wn−1)P(w_n|w_{n-1})P(wn​∣wn−1​)代替计算P(wn∣w1:n−1)P(w_n|w_{1:n-1})P(wn​∣w1:n−1​)。例如上述例子，计算P(言∣我最喜欢自然语)P(言|我最喜欢自然语)P(言∣我最喜欢自然语)可以通过计算P(言∣语)P(言|语)P(言∣语)来近似代替。 因此，当我们使用bigram模型来预测下一个单词的概率是，我们可以使用如下近似估算 P(wn∣w1:n−1)≈P(wn∣wn−1)P(w_n|w_{1:n-1})\\approx P(w_n|w_{n-1}) P(wn​∣w1:n−1​)≈P(wn​∣wn−1​) 实现上述假设，依赖于马尔科夫链（Markov）。 马尔科夫链 Reference N-gram Language Model","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://example.com/tags/model/"}]},{"title":"A Neural Probabilistic Language Model","slug":"NNML","date":"2021-11-17T07:42:48.000Z","updated":"2021-11-18T12:28:03.264Z","comments":true,"path":"2021/11/17/NNML/","link":"","permalink":"http://example.com/2021/11/17/NNML/","excerpt":"","text":"论文阅读 摘要部分：首先介绍了统计语言建模的一个难题，测试模型的单词序列可能与训练中看到的所有单词序列都不同，会导致维度诅咒（curse of dimensionality）问题的发生。","categories":[{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"paper","slug":"paper","permalink":"http://example.com/tags/paper/"}]},{"title":"nltk使用指南","slug":"nltk使用指南","date":"2021-11-16T12:28:53.000Z","updated":"2021-11-18T12:27:16.634Z","comments":true,"path":"2021/11/16/nltk使用指南/","link":"","permalink":"http://example.com/2021/11/16/nltk%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"安装方式 先使用pip安装pip install nltk，接着在命令行中输入如下句子，即可下载。 12import nltknltk.download() 如果报错，则修改Sever Index为http://www.nltk.org/nltk_data/即可下载任意数据。但也可能存在即使改了也不生效的情况，可以上网查找一波前辈们已经整理好的文件夹，按照他们的博文里面的方式解压存放。 这里注意解压之后把package改名为nltk_data后移动到相应的位置即可。 使用方法 查找指定单词上下文，使用函数concorance 1text1.concordance(&#x27;monstrous&#x27;) 查找文本中意思相近的词语，使用函数similar 1text1.similar(&#x27;monstrous&#x27;) 查找两个及以上词汇共用的上下文，使用函数common_context 1text2.common_contexts([&#x27;monstrous&#x27;,&#x27;very&#x27;]) 根据某一风格生成随机文本，使用函数generate 1text2.generate() 查看文本中某一词汇出现的次数 1text3.count(&#x27;smote&#x27;) 可以通过如下函数实现文本词汇丰富度测量 12def lexical_diversity(text): return len(text)/len(set(text)) 查看单个词汇在全文中占比 123def percentage(count,total): return 100*count/totalpercentage(text4.count(&#x27;a&#x27;),len(text4)) 查找给定单词在文本中的频率分布，使用FreqDist，该函数的返回值为一个字典。 12345fdist1=FreqDist(text1)fdist1#outputFreqDist(&#123;&#x27;,&#x27;: 18713, &#x27;the&#x27;: 13721, &#x27;.&#x27;: 6862, &#x27;of&#x27;: 6536, &#x27;and&#x27;: 6024, &#x27;a&#x27;: 4569, &#x27;to&#x27;: 4542, &#x27;;&#x27;: 4072, &#x27;in&#x27;: 3916, &#x27;that&#x27;: 2982, ...&#125;) 也可以绘制图text1文本最常见的50个词 接着可以查看文本中任意词汇出现的频率 1fdist1.freq(&#x27;very&#x27;) 通过bigrams可以实现词语两两搭配 1234[i for i in bigrams([&#x27;I&#x27;,&#x27;love&#x27;,&#x27;nlp&#x27;])]#output[(&#x27;I&#x27;, &#x27;love&#x27;), (&#x27;love&#x27;, &#x27;nlp&#x27;)] 查找词汇搭配使用collocations() 1text4.collocations()","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"nltk","slug":"nltk","permalink":"http://example.com/tags/nltk/"}]},{"title":"旅游计划","slug":"迪士尼","date":"2021-11-16T08:41:06.000Z","updated":"2021-11-17T07:38:36.064Z","comments":true,"path":"2021/11/16/迪士尼/","link":"","permalink":"http://example.com/2021/11/16/%E8%BF%AA%E5%A3%AB%E5%B0%BC/","excerpt":"","text":"期待迪士尼之行","categories":[{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}],"categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"},{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"},{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"supervise","slug":"supervise","permalink":"http://example.com/tags/supervise/"},{"name":"mechine learning","slug":"mechine-learning","permalink":"http://example.com/tags/mechine-learning/"},{"name":"nlp","slug":"nlp","permalink":"http://example.com/tags/nlp/"},{"name":"model","slug":"model","permalink":"http://example.com/tags/model/"},{"name":"paper","slug":"paper","permalink":"http://example.com/tags/paper/"},{"name":"nltk","slug":"nltk","permalink":"http://example.com/tags/nltk/"},{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}