{"meta":{"title":"evitcet's blog","subtitle":"","description":"just a bad code","author":"Evitcet","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2021-11-16T16:00:00.000Z","updated":"2021-11-28T09:25:50.662Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"一个不算有趣的人 喜欢读书和旅游"},{"title":"archives","date":"2021-11-28T09:25:50.701Z","updated":"2021-11-28T09:25:50.701Z","comments":true,"path":"archives/index.html","permalink":"http://example.com/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2021-11-28T09:25:50.746Z","updated":"2021-11-28T09:25:50.746Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-11-28T09:25:50.824Z","updated":"2021-11-28T09:25:50.824Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-11-28T09:25:50.839Z","updated":"2021-11-28T09:25:50.839Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"travel","date":"2021-11-29T02:59:37.285Z","updated":"2021-11-28T09:25:50.920Z","comments":true,"path":"tags/travel/index.html","permalink":"http://example.com/tags/travel/index.html","excerpt":"","text":""}],"posts":[{"title":"Huffman tree的python实现","slug":"Huffman-tree的python实现","date":"2021-11-30T13:08:31.000Z","updated":"2021-11-30T14:26:08.093Z","comments":true,"path":"2021/11/30/Huffman-tree的python实现/","link":"","permalink":"http://example.com/2021/11/30/Huffman-tree%E7%9A%84python%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"因为层次softmax用到了哈夫曼树，因此这里给出对应的代码实现，原理大家可以上网找一下，不过我相信应该都会的(≧▽≦) 首先构建一个哈夫曼树的结点，结点中需要存储的信息有： word_id : 结点编号 frequency : 节点权值（如果把结点当作单词，可以理解为文本里该单词出现的频率） left child : 左子树 right child : 右子树 father : 父结点 Huffman code : 哈夫曼码 path : 根到叶子结点的中间结点id（记录走到该结点经过的路径） 现在就可以搭建一棵哈夫曼树的雏形： 1234567891011121314class HuffmanTree: def __init__(self, wordid_frequency_dict): # 获取所有的结点id以及对应权值组成的字典 self.word_count = len(wordid_frequency_dict) # 结点个数，也即单词数量 self.wordid_code = dict() # 找到该结点的哈夫曼码 self.wordid_path = dict() # 找到该结点经过的中间结点编号 self.root = None # 根节点 unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # 未合并节点list self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] # 存储所有的叶子节点和中间节点 # 构建huffman tree self.build_tree(unmerge_node_list) # 生成huffman code self.generate_huffman_code_and_path() 这里给出哈夫曼编码结点的代码： 123456789class HuffmanNode: def __init__(self, word_id, frequency): self.word_id = word_id # 叶子结点存词对应的id, 父（中间）结点存父结点对应的id self.frequency = frequency # 存权值，也即单词频次 self.left_child = None self.right_child = None self.father = None self.Huffman_code = [] # 哈夫曼码（左1右0） self.path = [] # 根到叶子结点点的中间节点id 构建一棵哈夫曼树的最重要步骤是合并当前权值最小的两个结点，这两个结点可以合并成为一个新的父结点，也可以称为中间结点，最后合并成的中间结点是根结点。 1234567891011121314def merge_node(self, node1, node2): sum_frequency = node1.frequency + node2.frequency # 求两个结点的权值之和，将其作为这两个结点的父结点权值 mid_node_id = len(self.huffman) # 将现有的哈夫曼结点数作为父结点的id father_node = HuffmanNode(mid_node_id, sum_frequency) #生成一个新的结点，即父结点。 # 左子树存放权值大的结点，右边存放权值小的结点 if node1.frequency &gt;= node2.frequency: father_node.left_child = node1 father_node.right_child = node2 else: father_node.left_child = node2 father_node.right_child = node1 self.huffman.append(father_node) # 将新生成的哈夫曼结点添加到已有结点中 return father_node # 返回生成的父结点 接着我们要把树给搭起来，哈夫曼建树的原则一直是不断合并最小结点，直到合并结束。每次参与合并的两个最小结点不会再参与之后的合并。 1234567891011121314151617181920212223242526272829303132def build_tree(self, node_list): while len(node_list) &gt; 1: # 这里i1和i2的初值其实可以随便选取，每轮结束之后，两个值代表的一定都是现有的仍未合并的结点中概率最小的两个结点 i1 = 0 # 当前概率最小的结点（也可以随意赋初值，代码方便一般都为0） i2 = 1 # 当前概率第二小的结点（也可以随意赋初值，代码方便一般都为1） # 如果i2代表的结点权值大于i1代表的结点权值，则交换两个结点 if node_list[i2].frequency &lt; node_list[i1].frequency: [i1, i2] = [i2, i1] # 依次遍历余下结点，如果存在其他结点的权值小于i2结点，则将i2结点的id更改成该结点，更改后还要将新的i2结点权值与原有的i1结点权值做比较，如果该结点的权值也小于i1结点的权值，则将i1结点的id更新为该结点（也是更新后的i2结点） for i in range(2, len(node_list)): if node_list[i].frequency &lt; node_list[i2].frequency: i2 = i if node_list[i2].frequency &lt; node_list[i1].frequency: [i1, i2] = [i2, i1] father_node = self.merge_node(node_list[i1], node_list[i2]) # 合并最小的两个结点，生成父结点 # 如果i1结点的编号小于i2结点，则先弹出i2结点，再弹出i1结点，反之亦然，但两个结点不可能是同一个编号，左右子树结点的编号一定不同。 if i1 &lt; i2: node_list.pop(i2) node_list.pop(i1) elif i1 &gt; i2: node_list.pop(i1) node_list.pop(i2) else: raise RuntimeError(&#x27;i1 should not be equal to i2&#x27;) node_list.insert(0, father_node) # 在第一位插入新节点 # 最后还留在结点列表中的结点的新的父结点就是根节点 self.root = node_list[0] 最后构建哈夫曼编码和路径 1234567891011121314151617181920212223242526272829def generate_huffman_code_and_path(self): # 用栈来存放各个结点，先存储根节点 stack = [self.root] # 判断栈中是否还有结点存在 while len(stack) &gt; 0: node = stack.pop() # 最先弹出根节点，其次弹出父结点，最后叶子结点 # 先顺着左子树走 while node.left_child or node.right_child: # 判断是否存在左子树或者右子树 code = node.Huffman_code path = node.path node.left_child.Huffman_code = code + [1] # 在其左子树结点的哈夫曼编码中添加对应的编码1 node.right_child.Huffman_code = code + [0] # 在其右子树结点的哈夫曼编码中添加对应的编码0 node.left_child.path = path + [node.word_id] # 在其左子树结点的哈夫曼编码中添加其对应的id node.right_child.path = path + [node.word_id] # 在其右子树结点的哈夫曼编码中添加其对应的id # 把没走过的右子树加入栈 stack.append(node.right_child) # 令当前结点为其左子树的结点 node = node.left_child # 当确定该节点的左子树全部走完之后，进入下述步骤。 word_id = node.word_id word_code = node.Huffman_code word_path = node.path self.huffman[word_id].Huffman_code = word_code self.huffman[word_id].path = word_path # 把节点计算得到的霍夫曼码、路径 写入词典的数值中 self.wordid_code[word_id] = word_code self.wordid_path[word_id] = word_path 这里也可以获取所有结点的正向和负向路径，即哈夫曼编码中1和0代表的结点 123456789101112131415# 获取所有词的正向节点id和负向节点id数组def get_all_pos_and_neg_path(self): positive = [] # 所有词的正向路径数组 negative = [] # 所有词的负向路径数组 for word_id in range(self.word_count): pos_id = [] # 存放一个词 路径中的正向节点id neg_id = [] # 存放一个词 路径中的负向节点id for i, code in enumerate(self.huffman[word_id].Huffman_code): if code == 1: pos_id.append(self.huffman[word_id].path[i]) else: neg_id.append(self.huffman[word_id].path[i]) positive.append(pos_id) negative.append(neg_id) return positive, negative 最后完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&quot;&quot;&quot; 构建霍夫曼树&quot;&quot;&quot;class HuffmanNode: def __init__(self, word_id, frequency): self.word_id = word_id self.frequency = frequency self.left_child = None self.right_child = None self.father = None self.Huffman_code = [] self.path = [] class HuffmanTree: def __init__(self, wordid_frequency_dict): self.word_count = len(wordid_frequency_dict) self.wordid_code = dict() self.wordid_path = dict() self.root = None unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in wordid_frequency_dict.items()] self.build_tree(unmerge_node_list) self.generate_huffman_code_and_path() def merge_node(self, node1, node2): sum_frequency = node1.frequency + node2.frequency mid_node_id = len(self.huffman) father_node = HuffmanNode(mid_node_id, sum_frequency) if node1.frequency &gt;= node2.frequency: father_node.left_child = node1 father_node.right_child = node2 else: father_node.left_child = node2 father_node.right_child = node1 self.huffman.append(father_node) return father_node def build_tree(self, node_list): while len(node_list) &gt; 1: i1 = 0 i2 = 1 if node_list[i2].frequency &lt; node_list[i1].frequency: [i1, i2] = [i2, i1] for i in range(2, len(node_list)): if node_list[i].frequency &lt; node_list[i2].frequency: i2 = i if node_list[i2].frequency &lt; node_list[i1].frequency: [i1, i2] = [i2, i1] # print(&#x27;i1,i2&#x27;,[i1,i2]) father_node = self.merge_node(node_list[i1], node_list[i2]) if i1 &lt; i2: node_list.pop(i2) node_list.pop(i1) elif i1 &gt; i2: node_list.pop(i1) node_list.pop(i2) else: raise RuntimeError(&#x27;i1 should not be equal to i2&#x27;) node_list.insert(0, father_node) self.root = node_list[0] def generate_huffman_code_and_path(self): stack = [self.root] while len(stack) &gt; 0: node = stack.pop() while node.left_child or node.right_child: code = node.Huffman_code path = node.path node.left_child.Huffman_code = code + [1] node.right_child.Huffman_code = code + [0] node.left_child.path = path + [node.word_id] node.right_child.path = path + [node.word_id] stack.append(node.right_child) node = node.left_child word_id = node.word_id word_code = node.Huffman_code word_path = node.path self.huffman[word_id].Huffman_code = word_code self.huffman[word_id].path = word_path self.wordid_code[word_id] = word_code self.wordid_path[word_id] = word_path def get_all_pos_and_neg_path(self): positive = [] negative = [] for word_id in range(self.word_count): pos_id = [] neg_id = [] for i, code in enumerate(self.huffman[word_id].Huffman_code): if code == 1: pos_id.append(self.huffman[word_id].path[i]) else: neg_id.append(self.huffman[word_id].path[i]) positive.append(pos_id) negative.append(neg_id) return positive, negativedef test(): word_frequency = &#123;0: 4, 1: 6, 2: 3, 3: 2, 4: 2&#125; print(&#x27;word_frequency&#x27;,word_frequency) tree = HuffmanTree(word_frequency) print(tree.wordid_code) print(tree.wordid_path) for i in range(len(word_frequency)): print(tree.huffman[i].path) print(tree.get_all_pos_and_neg_path())if __name__ == &#x27;__main__&#x27;: test() 撒花★,°:.☆(￣▽￣)/$:.°★ 。","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"Code","slug":"Code","permalink":"http://example.com/tags/Code/"}]},{"title":"Hierarchical Probabilistic论文阅读","slug":"CBOW论文阅读","date":"2021-11-29T02:44:07.000Z","updated":"2021-11-30T08:41:06.323Z","comments":true,"path":"2021/11/29/CBOW论文阅读/","link":"","permalink":"http://example.com/2021/11/29/CBOW%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","excerpt":"","text":"ceng最早在Hierarchical Probabilistic Neural Network Language Model一文中提出来，其主要目的是为了改进NNML训练代价过大的问题，并且最后将时间复杂度缩小到 ∣V∣/log2∣V∣|V|/log_2|V|∣V∣/log2​∣V∣。 层次二叉树的使用最早在Hierarchical Probabilistic Neural Network Language Model一文中提出来，其主要目的是为了改进NNML训练代价过大的问题，并且最后将计算的时间复杂度缩小。 导读 本文提出了一种更快速的神经概率语言模型。基本思想是将基本思想是把一个词的层次描述成一个 O(log∣V∣)O(log|V|)O(log∣V∣) 决策序列，并学会如何去做这些概率的选择，而不是直接像NNML一样直接预测每个单词的概率。 模型 原理 通常来说，模型的学习目标都是一致的，都是估计单词序列的联合概率。方法一般都是通过给出几个之前的单词(上下文)来估计下一个单词(目标单词)的条件概率来做到这一点，可以表现为如下形式： P(w1,...,wl)=∏tP(wt∣wt−1,...,wt−n+1)P(w_1,...,w_l)=\\prod _tP(w_t|w_{t-1},...,w_{t-n+1}) P(w1​,...,wl​)=t∏​P(wt​∣wt−1​,...,wt−n+1​) 其中 wtw_twt​ 表示单词在序列中的位置，且 wt∈Vw_t \\in Vwt​∈V ，VVV 表示词表，并且上述式子满足 ∑vf(v,wt−1,...,wt−n+1)=1\\sum _vf(v,w_{t-1},...,w_{t-n+1})=1∑v​f(v,wt−1​,...,wt−n+1​)=1 这一约束条件。 NNML同样也遵循上述的学习目标和约束条件，其首先输出的是给定上下文生成下一个单词 wtw_twt​ 的非归一化概率 yyy ，最后的输出是一个经过了softmax归一化后的 ∣V∣|V|∣V∣ 大小的向量，但在学习过程中，只有上下文单词被映射成词向量，而目标单词 wtw_twt​ 并没被映射。 在NNML论文中曾经提到过两种减小复杂度的做法，这里作者采用了第二种方式，即树结构。 首先将模型通过PoHMM变体为： f(wt,wt−1,...,wt−n+1)=e−g(wt,wt−1,...,wt−n+1)∑we−g(v,wt−1,...,wt−n+1)f(w_t,w_{t-1},...,w_{t-n+1})=\\frac{e^{-g(w_t,w_{t-1},...,w_{t-n+1})}}{\\sum _we^{-g(v,w_{t-1},...,w_{t-n+1})}} f(wt​,wt−1​,...,wt−n+1​)=∑w​e−g(v,wt−1​,...,wt−n+1​)e−g(wt​,wt−1​,...,wt−n+1​)​ 上式中，需要学习的是 −g(v,wt−1,...,wt−n+1)-g(v,w_{t-1},...,w_{t-n+1})−g(v,wt−1​,...,wt−n+1​) 可以使用能量（energy）一词来表示，而当序列 (v,wt−1,...,wt−n+1)(v,w_{t-1},...,w_{t-n+1})(v,wt−1​,...,wt−n+1​) 通顺可行时，g(v,wt−1,...,wt−n+1)g(v,w_{t-1},...,w_{t-n+1})g(v,wt−1​,...,wt−n+1​) 也会变小。 接着假设 FFF 是一个词向量矩阵，第 iii 行的 FiF_iFi​ 表示的是单词 iii 在词向量空间映射成的特征向量，则上述能量函数 ggg 可由特征向量 FiF_iFi​ 的第一次变换表示如下： g(v,wt−1,...,wt−n+1)=a′⋅tanh(c+Wx+UFv′)+bvg(v,w_{t-1},...,w_{t-n+1})=a&#x27;·tanh(c+Wx+UF&#x27;_v)+b_v g(v,wt−1​,...,wt−n+1​)=a′⋅tanh(c+Wx+UFv′​)+bv​ 这里的x'表示x的转置，a、ba、ba、b 和 ccc 均为参数向量，WWW 和 UUU 是权重矩阵 ，xxx 表示输入特征向量的拼接： x=(Fwt−1,...,Fwt−n+1)′x=(F_{w_{t-1}},...,F_{w_{t-n+1}})&#x27; x=(Fwt−1​​,...,Fwt−n+1​​)′ 令 hhh 为隐藏单元的数量（WWW 的行数），ddd 为隐藏向量的维度（FFF 的列数），则计算 fff 需要以下两步： 先计算 c+Wxc+Wxc+Wx ，需要 hd(n−1)hd(n-1)hd(n−1) 次乘积累加计算。 对于每一个 v∈Vv \\in Vv∈V ，计算 UFv′UF&#x27;_vUFv′​ 和 g(v,...)g(v,...)g(v,...)，前者需要 hdhdhd 次乘积累加计算，后者需要 hhh 次乘积累加计算。 综上计算 fff 所需要的时间为 hd(n−1)+∣V∣h(d+1)hd(n-1)+|V|h(d+1)hd(n−1)+∣V∣h(d+1) ，这里乘上 ∣V∣|V|∣V∣ 是因为要对此表里的每一个词做上述运算。 回顾一下先前的NNML表达式： P^(wt∣wt−1,...,wt−n+1)=eywt∑ieyi\\hat P(w_t|w_{t-1},...,w_{t-n+1})=\\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}} P^(wt​∣wt−1​,...,wt−n+1​)=∑i​eyi​eywt​​​ 上述表达式中的 y=b+Wx+Utanh(d+Hx)y=b+Wx+Utanh(d+Hx) y=b+Wx+Utanh(d+Hx) 下面是我的一些见解，如果有不同观点可以讨论： ggg 和 yyy 两个式子的最大差距在于 ggg 对输出做了和输入一样的词向量映射，而 hhh 仅仅计算输出的概率结果。 在 ggg 中省去了词向量到输出的直连，把 yyy 中的 WWW 项置为0，因此 yyy 中的 bbb 就是 ggg 中的 bvb_vbv​ ，计算时 ggg 也对每一个可能的 vvv 进行词向量的映射，取出对应的 FvF_vFv​ 进行单独计算，而不是直接给出每个单词的概率结果。 从上述的时间复杂度可以近似成为 O(∣V∣)O(|V|)O(∣V∣)，本文的目的就是将 O(∣V∣)O(|V|)O(∣V∣) 下降到 O(log∣V∣)O(log|V|)O(log∣V∣)。 层次二叉树 作者提出了一种思想提高基于类的统计语言模型的最大熵的计算速度。该思想并不直接计算 P(Y∣X)P(Y|X)P(Y∣X) ，而是先将所有可能的 YYY 定义一个集群分区，假设词表里的所有词可以分为 CCC 个类，那么定义函数 c(.)c(.)c(.) 为 YYY 到 CCC 的映射，因此可以写成： P(Y=y∣X=x)=P(Y=y∣C=c(y),X)P(C=c(y)∣X=x)P(Y=y|X=x)=P(Y=y|C=c(y),X)P(C=c(y)|X=x) P(Y=y∣X=x)=P(Y=y∣C=c(y),X)P(C=c(y)∣X=x) 其中CCC 和 YYY 中的值是一一对应的。 我们可以使用哈夫曼树实现上述思想，哈夫曼树就不做赘述了。现在假设去左子树的路径均为0，去右子树均为1，假设我们想要得到6，原先的办法需要查找4次，但是现在之需要查找1次，路径为0，以此类推，4的路径为11，1的路径为100，2的路径为111。 在以前的方法中，我们需要计算出所有词的softmax结果，再去寻找概率最大的值。softmax实际上也是多分类的logistic回归，这里我们使用logistics来判断在哈夫曼树中我们走左子树还是右子树，它的值也就是走某一条路的概率。 假设我们用1比特向量序列 b1(v),...,bm(V)b_1(v),...,b_m(V)b1​(v),...,bm​(V) 表示选择2要走路径，那么该序列应该为b1(v)=1,b2(v)=1,b3(v)=1b_1(v)=1,b_2(v)=1,b_3(v)=1b1​(v)=1,b2​(v)=1,b3​(v)=1。 因此原本的表达式可以改写为： P(v∣wt−1,...,wt−n+1)=∏jmP(bj(v)∣b1(v),...bj−1(v),wt−1,...,wt−n+1)P(v|w_{t-1},...,w_{t-n+1})=\\prod ^m _j P(b_j(v)|b_1(v),...b_{j-1}(v),w_{t-1},...,w_{t-n+1}) P(v∣wt−1​,...,wt−n+1​)=j∏m​P(bj​(v)∣b1​(v),...bj−1​(v),wt−1​,...,wt−n+1​) 那么假设上述是一棵平衡二叉树，那么位向量的最大长度只有 ⌈log2∣V∣⌉⌈log2|V|⌉⌈log2∣V∣⌉。 综上所述，我们可以使用模型来预测 P(b∣node,wt−1,...,wt−n+1)P(b|node,w_{t-1},...,w_{t-n+1})P(b∣node,wt−1​,...,wt−n+1​) ，其中node对应指定层次结构中节点的位序列，b是0或者1，即对应于两个子节点其中之一。这种情况下模型只需要进行两种预测，而不需要进行 ∣V∣|V|∣V∣ 种。 最后，完整的表达式可以写成 P(b=1∣node,wt−1,...,wt−n+1)=sigmoid(αnode+β′⋅tanh(c+Wx+UNnode))P(b=1|node,w_{t-1},...,w_{t-n+1})=sigmoid(\\alpha _{node}+\\beta &#x27;·tanh(c+Wx+UN_{node})) P(b=1∣node,wt−1​,...,wt−n+1​)=sigmoid(αnode​+β′⋅tanh(c+Wx+UNnode​)) 这里的 xxx 依旧是上下文词向量的拼接， sigmoid(y)=1/1+exp(−y)sigmoid(y)=1/1+exp(-y)sigmoid(y)=1/1+exp(−y) ，αi\\alpha_iαi​ 和 bvb_vbv​ 是一样偏置，其余参数向量也同上。","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/tags/Paper/"}]},{"title":"NNML的pytorch应用","slug":"NNML的pytorch应用","date":"2021-11-28T09:38:36.000Z","updated":"2021-11-29T08:57:27.218Z","comments":true,"path":"2021/11/28/NNML的pytorch应用/","link":"","permalink":"http://example.com/2021/11/28/NNML%E7%9A%84pytorch%E5%BA%94%E7%94%A8/","excerpt":"","text":"NNML的pytorch应用 本文主要介绍基于pytorch框架下的NNML解决的一个问题：预测给定序列的下一个单词。 导入需要用到的库 123456import torchimport torch.nn as nnimport torch.optim as optimimport torch.utils.data as Datadevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 处理数据 简单起见，本文提供4个长度一致的短句，如果长度不一致需要对句子进行填充或者是截断。 1234sentences = [&quot;i like dog&quot;, &quot;i love coffee&quot;, &quot;i hate milk&quot;,&quot;he like cat&quot;]word_list = &quot; &quot;.join(sentences).split()word_list = list(set(word_list))#将句子整理成无重复单词的词表#[&#x27;cat&#x27;, &#x27;love&#x27;, &#x27;milk&#x27;, &#x27;coffee&#x27;, &#x27;hate&#x27;, &#x27;he&#x27;, &#x27;i&#x27;, &#x27;dog&#x27;, &#x27;like&#x27;] 首先构建字典，word2id是每个单词对应的索引，id2word是每个索引对应的单词，n_class是词表的总个数，也即论文里的|V|。 12345678word2id = &#123;w: i for i, w in enumerate(word_list)&#125;# &#123;&#x27;cat&#x27;: 0,&#x27;love&#x27;: 1,&#x27;milk&#x27;: 2,&#x27;coffee&#x27;: 3,&#x27;hate&#x27;: 4,&#x27;he&#x27;: 5,&#x27;i&#x27;: 6,&#x27;dog&#x27;: 7,&#x27;like&#x27;: 8&#125;id2word = &#123;i: w for i, w in enumerate(word_list)&#125;# &#123;0: &#x27;cat&#x27;,1: &#x27;love&#x27;,2: &#x27;milk&#x27;,3: &#x27;coffee&#x27;,4: &#x27;hate&#x27;,5: &#x27;he&#x27;,6: &#x27;i&#x27;,7: &#x27;dog&#x27;,8: &#x27;like&#x27;&#125;n_class = len(word2id) # number of Vocabulary# 9 其次设置模型所需的基本参数。 n_step：已知长序列的单词个数 m：embedding size，即每个单词对应的特征向量维度 n_hidden：模型中隐藏层的个数 batch_size：每组数据的个数 1234n_step = 2 # number of steps, n-1 in papern_hidden = 2 # number of hidden size, h in paperm = 2 # embedding size, m in paperbatch_size = 1 最后根据上述参数构建Dataset和DataLoader。 12345678910111213141516171819def make_data(): input_batch = [] target_batch = [] for sen in sentences: word = sen.split() # space tokenizer 对每个句子单独处理 input = [word_dict[n] for n in word[:-1]] # create (1~n-1) as input 获取前n_step个单词对应的索引 target = word_dict[word[-1]] # create (n) as target, We usually call this &#x27;casual language model&#x27; 获得需要预测的第n个单词的索引 input_batch.append(input) target_batch.append(target) return input_batch, target_batch#input_batch: [[6,8],[6,1],[6,4],[5,8]]#output_batch: [7,3,2,0]dataset = Data.TensorDataset(input_batch, target_batch)loader = Data.DataLoader(dataset, batch_size, False) 构建模型 NNML中一共有6个参数，表达式为y=b+Wx+Utanh(d+Hx) 1234567891011121314151617class NNML(nn.Module): def __init__(self): super(NNLM, self).__init__() self.C = nn.Embedding(n_class, m) self.H = nn.Linear(n_step * m, n_hidden, bias=False) self.d = nn.Parameter(torch.ones(n_hidden)) self.U = nn.Linear(n_hidden, n_class, bias=False) self.W = nn.Linear(n_step * m, n_class, bias=False) self.b = nn.Parameter(torch.ones(n_class)) def forward(self, X): # X : [batch_size,n_step] X = self.C(X) # X : [batch_size, n_step, m] 将前n-1个单词映射成维度为m的特征向量 X = X.view(-1, n_step * m) # [batch_size, n_step * m] tanh = torch.tanh(self.d + self.H(X)) # [batch_size, n_hidden] output = self.b + self.W(X) + self.U(tanh) # [batch_size, n_class] return output 训练数据 1234567891011121314151617model = NNLM().to(device) #定义模型criterion = nn.CrossEntropyLoss().to(device) #定义损失函数optimizer = optim.Adam(model.parameters(), lr=0.001) #定义优化器for epoch in range(5000): for x,y in loader: x,y =x.to(device),y.to(device) optimizer.zero_grad() output = model(x) # output : [batch_size, n_class], target_batch : [batch_size] loss = criterion(output, y) loss.backward() optimizer.step() if (epoch + 1) % 1000 == 0: print(&#x27;Epoch:&#x27;, &#x27;%04d&#x27; % (epoch + 1), &#x27;cost =&#x27;, &#x27;&#123;:.6f&#125;&#x27;.format(loss)) 测试数据 123456ctxid=0for x,y in loader: x,y =x.to(device),y.to(device) predict = model(x).data.max(1, keepdim=True)[1] print(sentences[ctxid].split()[:2], &#x27;-&gt;&#x27;, id2word[predict.squeeze().item()]) ctxid+=1 最后得出输出结果 1234[&#x27;i&#x27;, &#x27;like&#x27;] -&gt; dog[&#x27;i&#x27;, &#x27;love&#x27;] -&gt; coffee[&#x27;i&#x27;, &#x27;hate&#x27;] -&gt; milk[&#x27;he&#x27;, &#x27;like&#x27;] -&gt; cat 以上就实现了一次神经网络模型对指定序列的下一个词汇预测。","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"Code","slug":"Code","permalink":"http://example.com/tags/Code/"}]},{"title":"隐马尔可夫模型","slug":"HHM","date":"2021-11-18T02:14:58.000Z","updated":"2021-11-29T08:49:18.539Z","comments":true,"path":"2021/11/18/HHM/","link":"","permalink":"http://example.com/2021/11/18/HHM/","excerpt":"","text":"0x01 Hidden Markov Model 例子 假设一共有如图所示的3个盒子，分别为A、B和C，每个盒子里一共有两种颜色的球，分别为蓝色和红色。 现在要求小y需要抽球，抽球的方式如下： 以等概率挑选任意1个盒子，再从该盒子里抽取任意1个球，记录颜色之后放回。 接着从当前盒子转移到下一个盒子。（为了简单起见，转移的概率也是相等的13\\frac{1}{3}31​） 从下一个盒子中取出1个球，记录颜色后放回。 小y重复上述抽球行为9次后，得到了一串球颜色的序列，为 O=(r,r,r,b,r,b,r,b,b)O=(r,r,r,b,r,b,r,b,b)O=(r,r,r,b,r,b,r,b,b)。 我们作为这场抽球游戏的观测者，只能观测到小y抽取到的球颜色，并不能观测到小y到底从哪些盒子里取的球。 那么在上述的例子中，存在两个随机序列。在隐马尔科夫模型中，可观测到的序列OOO叫观测（随机）序列，也称为可见状态链；假设选中的盒子序列为III，那么不能被观测到RRR叫做状态（随机）序列，也称为隐含状态链。 假设选中盒子的序列为I={A,A,B,C,B,A,C,A,B}I=\\{A,A,B,C,B,A,C,A,B\\}I={A,A,B,C,B,A,C,A,B}，那么可以结合上述条件绘制出隐马尔科夫模型： 图中第一排表示的是隐含状态链，第二排是可见状态链，绿色箭头表示的是从一个隐含状态到一个可见状态的输出，黄色箭头表示的是从一个隐含状态到下一个隐含状态的输出。 根据条件，我们也可以得到状态集合、观测集合、序列长度以及模型的三要素。 盒子对应着状态，因此状态集合是： Q={A,B,C},N=3Q=\\{A,B,C\\}, N=3 Q={A,B,C},N=3 球的颜色对应着观测，因此观测集合是： V={r,b},M=2V=\\{r,b\\},M=2 V={r,b},M=2 状态序列和观测序列长度均为T=9T=9T=9。 隐马尔科夫模型的模型三要素为初始状态分布πππ，状态转移概率分布AAA和观测概率分布BBB 初始状态分布πππ对应着最初的等概率取盒子： π=(13,13,13)\\pi=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}) π=(31​,31​,31​) 状态转移概率AAA对应着每个盒子转移到下一个盒子的概率分布，本例中均为等概率： [131313131313131313]\\begin{bmatrix} \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3}\\\\ \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3}\\\\ \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} \\end{bmatrix} ⎣⎢⎡​31​31​31​​31​31​31​​31​31​31​​⎦⎥⎤​ 观测概率分布 BBB 对应着每个盒子里各种颜色球的概率分布： [154545152535]\\begin{bmatrix} \\frac{1}{5} &amp; \\frac{4}{5} \\\\ \\frac{4}{5} &amp; \\frac{1}{5} \\\\ \\frac{2}{5} &amp; \\frac{3}{5} \\end{bmatrix} ⎣⎢⎡​51​54​52​​54​51​53​​⎦⎥⎤​ 下面给出HMM的官方定义，如果上述例子已经看懂了，也可以直接跳过定义部分。 定义 隐马尔科夫模型（hidden Markov model，HHM）是一种基于监督学习的模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 定义：马尔科夫链是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测，从而产生观测随机序列的过程。隐马尔科夫链随机生成的状态的序列，成为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。 隐马尔科夫模型是由初始概率分布、状态转移概率分布以及观测概率分布确定，具体形式定义如下： 假设QQQ是所有可能的状态集合，VVV是所有可能的观测集合： Q={q1,q2,...qN},V={v1,v2,...,vM}Q=\\{q_1,q_2,...q_N \\},V=\\{v_1,v_2,...,v_M\\} Q={q1​,q2​,...qN​},V={v1​,v2​,...,vM​} NNN是对应的状态数，MMM是对应的观测数。 假设 III 是长度为 TTT 的状态序列，OOO是对应的观测序列： I=(i1,i2,...,iT),O=(o1,o2,...oT)I=(i_1,i_2,...,i_T),O=(o_1,o_2,...o_T) I=(i1​,i2​,...,iT​),O=(o1​,o2​,...oT​) AAA是状态转移概率矩阵： A=[aij]N×NA=[a_{ij}]_{N\\times N} A=[aij​]N×N​ 其中， aij=P(it+1=qj∣it=qi),i=1,2,...,N;j=1,2,...,Na_{ij}=P(i_{t+1}=q_j|i_t=q_i),i=1,2,...,N;j=1,2,...,N aij​=P(it+1​=qj​∣it​=qi​),i=1,2,...,N;j=1,2,...,N aija_{ij}aij​表示在时刻ttt处于状态qiq_iqi​的条件下，在时刻t+1t+1t+1转移到状态qjq_jqj​的概率。 BBB是观测概率矩阵： B=[bj(k)]N×MB=[b_j(k)]_{N\\times M} B=[bj​(k)]N×M​ 其中， bj(k)=P(ot=vk∣it=qj),j=1,2...,M;j=1,2...Nb_j(k)=P(o_t=v_k|i_t=q_j),j=1,2...,M;j=1,2...N bj​(k)=P(ot​=vk​∣it​=qj​),j=1,2...,M;j=1,2...N bj(k)b_j(k)bj​(k)表示处在状态qjq_jqj​条件下生成观测vkv_kvk​的概率， π\\piπ是初始状态概率向量： π=(πi)\\pi=(\\pi_i) π=(πi​) 其中， πi=P(i1=qi),i=1,2,...,N\\pi_i=P(i_1=q_i),i=1,2,...,N πi​=P(i1​=qi​),i=1,2,...,N πi\\pi_iπi​表示时刻t=1t=1t=1处于状态qiq_iqi​的概率。 A,B,πA,B,\\piA,B,π 共同构成了隐马尔科夫模型的三要素，可用三元组表示为λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)。 基本假设 由上述也可以得出，隐马尔科夫模型作了两个基本假设： 齐次马尔科夫假设，即隐藏的马尔科夫链在任意时刻t的状态只依赖前一时刻的状态，与其他时刻的状态以及观测无关，也与时刻t本身无关： P(it∣it−1,ot−1,...,i1,o1)=P(it∣it−1),t=1,2,...,TP(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,T P(it​∣it−1​,ot−1​,...,i1​,o1​)=P(it​∣it−1​),t=1,2,...,T 观测独立性假设，即任意时刻的观测只依赖于该时刻t的马尔科夫链的状态，与其他时刻及状态无关： P(ot∣iT,oT,iT−1,oT−1,...,it+1,ot+1,it,it−1,ot−1,...,i1,o1)=P(it∣it−1),t=1,2,...,T P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1}),t=1,2,...,T P(ot​∣iT​,oT​,iT−1​,oT−1​,...,it+1​,ot+1​,it​,it−1​,ot−1​,...,i1​,o1​)=P(it​∣it−1​),t=1,2,...,T 用上述的例子简单解释一下两个基本性质： 齐次马尔科夫假设可以理解为，在抽小球的过程中，决定下一时刻选什么箱子，只和前一时刻状态有关系： 假设在时刻ttt的时候选中的箱子为A，即it=Ai_t=Ait​=A，那么在时刻t+1t+1t+1时选到B箱子的概率为p(it+1=B∣it=A)=13p(i_{t+1}=B|i_{t}=A)=\\frac{1}{3}p(it+1​=B∣it​=A)=31​，选到C箱子的概率为p(it+1=C∣it=A)=13p(i_{t+1}=C|i_{t}=A)=\\frac{1}{3}p(it+1​=C∣it​=A)=31​，选到A箱子的概率为p(it+1=A∣it=A)=13p(i_{t+1}=A|i_{t}=A)=\\frac{1}{3}p(it+1​=A∣it​=A)=31​。转移概率的值仅和上一状态有关，和其他无关。 观测独立性假设可以理解为，在抽小球的过程中，决定当前时刻抽中的球的颜色只和当前时刻的状态有关： 假设在时刻ttt选中的箱子为AAA，也即it=Ai_t=Ait​=A，那么在当前时刻抽中的红球的概率为P(oi=r∣it=A)=15P(o_i=r|i_t=A)=\\frac{1}{5}P(oi​=r∣it​=A)=51​，在当前时刻抽中的蓝球的概率为P(oi=b∣it=A)=45P(o_i=b|i_t=A)=\\frac{4}{5}P(oi​=b∣it​=A)=54​。当前时刻观测概率的值只和当前状态有关，和其他值无关。 0x02 三个问题（三种算法） 在介绍每一种算法之前，我都会给出算法的定义，再用小球问题解释这个算法。 假设盒子和球条件以及规则不变，这次小y取了三次球，获得的观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r) 概率计算算法 在我们已知最终取到的小球观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r)和模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)中的所有值的情况下，那我们可以使用概率计算算法获得在模型λ\\lambdaλ的条件下获得观测序列OOO的概率，即P(O∣λ)P(O|\\lambda)P(O∣λ)。概率计算算法共有三种，分别为： 直接计算法 前向算法 后向算法 直接计算法 算法 这种算法最直接，就是按照概率公式直接计算，先列举出所有可能的长度为TTT的状态序列I=(i1,i2,...,iN)I=(i_1,i_2,...,i_N)I=(i1​,i2​,...,iN​)，接着求出III和QQQ的联合概率P(O,I∣λ)P(O,I|\\lambda)P(O,I∣λ)，最后依次累加所有状态序列下的联合概率求得P(O∣λ)P(O|\\lambda)P(O∣λ)。注：这里加上λ\\lambdaλ只是为了注明模型条件，而不是计算条件概率。 例子 已知观测序列的长度T=3T=3T=3，状态个数N=3N=3N=3，计算可以得知III一共有NT=33=27N^T=3^3=27NT=33=27种排列方式。将每种排列方式下的P(I∣λ)P(I|\\lambda)P(I∣λ)和P(O∣I,λ)P(O|I,\\lambda)P(O∣I,λ)相乘即可求出P(O,I∣λ)P(O,I|\\lambda)P(O,I∣λ)。 这里假设I1=(i1=A,i2=A,i3=A)I_1=(i_1=A,i_2=A,i_3=A)I1​=(i1​=A,i2​=A,i3​=A)，那么: P(I1∣λ)={π,ai1,ai2}={13,13,13}P(I_1|\\lambda)=\\{\\pi,a_{i_1},a_{i_2}\\}=\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\} P(I1​∣λ)={π,ai1​​,ai2​​}={31​,31​,31​} 已知O=(o1=r,o2=b,o3=r)O=(o_1=r,o_2=b,o_3=r)O=(o1​=r,o2​=b,o3​=r)，那么： P(O∣I1λ)={bi1(o1),bi2(o2),bi3(o3)}={P(o1=r∣i1=A),P(o2=b∣i2=A),P(o3=r∣i3=A)}={15,45,15}\\begin{aligned} P(O|I_1\\lambda)&amp;=\\{b_{i_1}(o_1),b_{i_2}(o_2),b_{i_3}(o_3)\\}\\\\&amp;=\\{P(o_1=r|i_1=A),P(o_2=b|i_2=A),P(o_3=r|i_3=A)\\}\\\\&amp;=\\{\\frac{1}{5},\\frac{4}{5},\\frac{1}{5}\\} \\end{aligned}P(O∣I1​λ)​={bi1​​(o1​),bi2​​(o2​),bi3​​(o3​)}={P(o1​=r∣i1​=A),P(o2​=b∣i2​=A),P(o3​=r∣i3​=A)}={51​,54​,51​}​ 则联合概率P(O,I1∣λ)P(O,I_1|\\lambda)P(O,I1​∣λ)为： P(O,I1∣λ)=P(O∣I1,λ)P(I1∣λ)=13∗15∗13∗45∗13∗15=63375 \\begin{aligned} P(O,I_1|\\lambda)&amp;=P(O|I_1,\\lambda)P(I_1|\\lambda)\\\\&amp;=\\frac{1}{3}*\\frac{1}{5}*\\frac{1}{3}*\\frac{4}{5}*\\frac{1}{3}*\\frac{1}{5}\\\\&amp;=\\frac{6}{3375} \\end{aligned}P(O,I1​∣λ)​=P(O∣I1​,λ)P(I1​∣λ)=31​∗51​∗31​∗54​∗31​∗51​=33756​​ 这一步的时间复杂度为O(T)O(T)O(T)。 最后按照上述方式，依次计算出P(O,I2∣λ),...,P(O,I17∣λ)P(O,I_2|\\lambda),...,P(O,I_{17}|\\lambda)P(O,I2​∣λ),...,P(O,I17​∣λ)，累加后即得到P(O∣λ)P(O|\\lambda)P(O∣λ)： P(O∣λ)=∑i=127P(O∣Ii,λ)P(I∣λ)=3923375P(O|\\lambda)=\\sum_{i=1}^{27}P(O|I_i,\\lambda)P(I|\\lambda)=\\frac{392}{3375} P(O∣λ)=i=1∑27​P(O∣Ii​,λ)P(I∣λ)=3375392​ 但上述算法的计算量很大，这里很容易推得，复杂度是O(TNT)O(TN^T)O(TNT)阶。 前向算法 前向算法的定义如下： 给定马尔可夫模型λ\\lambdaλ，定义到时刻ttt部分观测序列为o1,o2,..oto_1,o_2,..o_to1​,o2​,..ot​且状态为qiq_iqi​的概率为前向概率，记作 αt(i)=P(o1,o2,...,ot,it=qi∣λ)\\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\\lambda) αt​(i)=P(o1​,o2​,...,ot​,it​=qi​∣λ) 可以递推地求得前向概率αt(i)\\alpha_t(i)αt​(i)以及观测概率P(O∣λ)P(O|\\lambda)P(O∣λ)。 算法 先计算初值： α1(i)=πibi(o1),i=1,2,...,N\\alpha_1(i)=\\pi_ib_i(o_1), i=1,2,...,N α1​(i)=πi​bi​(o1​),i=1,2,...,N 通过初始化前向概率，得到初始时刻的状态i1=qii_1=q_ii1​=qi​和观测o1o_1o1​的联合概率。 再递推，对t=1,2,...,T−1t=1,2,...,T-1t=1,2,...,T−1： αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2..,N\\alpha_{t+1}(i)=\\left [ \\sum_{j=1}^{N}\\alpha _t(j)a_{ji} \\right ]b_i(o_{t+1}),i=1,2..,N αt+1​(i)=[j=1∑N​αt​(j)aji​]bi​(ot+1​),i=1,2..,N 得出到时刻ttt观测到o1,o2,...,oto_1,o_2,...,o_to1​,o2​,...,ot​并在时刻ttt处于状态qjq_jqj​，而在时刻t+1t+1t+1达到状态qiq_iqi​的联合概率。这里值得注意的是，ajia_{ji}aji​中我们已知的是it+1=qii_{t+1}=q_iit+1​=qi​，未知的部分是it=qji_t=q_jit​=qj​，即由当前状态往前推。 最后得出： P(O∣λ)=∑i=1NαT(i)P(O|\\lambda)=\\sum^N_{i=1}\\alpha_T(i) P(O∣λ)=i=1∑N​αT​(i) 例子 前向算法理解起来不难，但是大部分公式都很繁杂，这里依然用上述的O=(r,b,r)O=(r,b,r)O=(r,b,r)和λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，方便起见使用编号1、2、31、2、31、2、3代替编号A、B、C。 现在我们需要求初值，也即时刻t=1t=1t=1时候的α1(i)\\alpha_1(i)α1​(i)结果。显然，时刻t=1t=1t=1，可能取到的盒子有1、2、31、2、31、2、3三种，这也即α1(1),α1(2),α1(3)\\alpha_1(1),\\alpha_1(2),\\alpha_1(3)α1​(1),α1​(2),α1​(3)。 先假设t=1t=1t=1时取到了111号盒子，即$q_1=1，根据观测序列1，根据观测序列1，根据观测序列o_1=r$，可以得到： α1(1)=P(o1,i1=q1∣λ)=P(o1=r∣i1=q1,λ)P(i1=q1∣λ)=15∗13=115\\begin{aligned} \\alpha_1(1)&amp;=P(o_1,i_1=q_1|\\lambda)\\\\&amp;=P(o_1=r|i_1=q_1,\\lambda)P(i_1=q_1|\\lambda)\\\\&amp;=\\frac{1}{5}*\\frac{1}{3}=\\frac{1}{15} \\end{aligned}α1​(1)​=P(o1​,i1​=q1​∣λ)=P(o1​=r∣i1​=q1​,λ)P(i1​=q1​∣λ)=51​∗31​=151​​ 同理可得α1(2)=415,α1(3)=215\\alpha_1(2)=\\frac{4}{15},\\alpha_1(3)=\\frac{2}{15}α1​(2)=154​,α1​(3)=152​。 在时刻t=2t=2t=2，可能取到的盒子的联合概率，即α2(1),α2(2),α2(3)\\alpha_2(1),\\alpha_2(2),\\alpha_2(3)α2​(1),α2​(2),α2​(3)。 假设t=2t=2t=2取到了222号盒子，即q2=2q_2=2q2​=2，根据已知观测序列o1=r,o2=bo_1=r,o_2=bo1​=r,o2​=b，可以得到： α2(2)=P(o1=r,o2=b,i2=q2∣λ)=[α1(1)a12+α1(2)a22+α1(3)a32]∗P(o2=b∣i2=q2,λ)=[115∗13+415∗13+215∗13]∗15=7225\\begin{aligned} \\alpha_2(2)&amp;=P(o_1=r,o_2=b,i_2=q_2|\\lambda)\\\\&amp;=[\\alpha _1(1)a_{12}+\\alpha _1(2)a_{22}+\\alpha _1(3)a_{32}]*P(o_2=b|i_2=q_2,\\lambda)\\\\&amp;=[\\frac{1}{15}*\\frac{1}{3}+\\frac{4}{15}*\\frac{1}{3}+\\frac{2}{15}*\\frac{1}{3}]*\\frac{1}{5}=\\frac{7}{225} \\end{aligned}α2​(2)​=P(o1​=r,o2​=b,i2​=q2​∣λ)=[α1​(1)a12​+α1​(2)a22​+α1​(3)a32​]∗P(o2​=b∣i2​=q2​,λ)=[151​∗31​+154​∗31​+152​∗31​]∗51​=2257​​ 同理可得α2(1)=28225,α2(3)=21225\\alpha_2(1)=\\frac{28}{225},\\alpha_2(3)=\\frac{21}{225}α2​(1)=22528​,α2​(3)=22521​ 同理计算出α3(1)=563375,α3(3)=2243375,α3(3)=1123375\\alpha_3(1)=\\frac{56}{3375},\\alpha_3(3)=\\frac{224}{3375},\\alpha_3(3)=\\frac{112}{3375}α3​(1)=337556​,α3​(3)=3375224​,α3​(3)=3375112​，累加后得到P(O∣λ)=3923375P(O|\\lambda)=\\frac{392}{3375}P(O∣λ)=3375392​。 综上，前向算法的时间复杂度为O(N2T)O(N^2T)O(N2T)。 后向算法 后向算法和前向算法基本一致，定义如下： 给定马尔可夫模型λ\\lambdaλ，定义在时刻ttt状态为qiq_iqi​的条件下，从t+1t+1t+1到TTT的部分观测序列为ot+1,ot+1,..oTo_{t+1},o_{t+1},..o_Tot+1​,ot+1​,..oT​的概率为后向概率，记作 βt(i)=P(ot+1,ot+2,...,oT∣it=qi,λ)\\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\\lambda) βt​(i)=P(ot+1​,ot+2​,...,oT​∣it​=qi​,λ) 可以递推地求得后向概率βt(i)\\beta_t(i)βt​(i)以及观测概率P(O∣λ)P(O|\\lambda)P(O∣λ)。 算法 先计算终值： βT(i)=1,i=1,2,...,N\\beta_T(i)=1, i=1,2,...,N βT​(i)=1,i=1,2,...,N 再递推，对t=T−1,T−2,...,1t=T-1,T-2,...,1t=T−1,T−2,...,1： βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2..,N\\beta_{t}(i)=\\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),i=1,2..,N βt​(i)=j=1∑N​aij​bj​(ot+1​)βt+1​(j),i=1,2..,N 其中aija_{ij}aij​表示P(it+1=qj∣it=qi)P(i_{t+1}=q_j|i_t=q_i)P(it+1​=qj​∣it​=qi​) 最后得出： P(O∣λ)=∑i=1Nπibi(o1)β1(i)P(O|\\lambda)=\\sum^N_{i=1}\\pi_ib_i(o_{1})\\beta_1(i) P(O∣λ)=i=1∑N​πi​bi​(o1​)β1​(i) 例子 这里依然用上述的O=(r,b,r)O=(r,b,r)O=(r,b,r)和λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，使用编号1、2、31、2、31、2、3代替编号A、B、C​。 假设T=3T=3T=3时刻取到的盒子为1​号盒，即q3=1q_3=1q3​=1。可得β3(1)=1\\beta_3(1)=1β3​(1)=1，同理β3(2)=1,β3(3)=1\\beta_3(2)=1,\\beta_3(3)=1β3​(2)=1,β3​(3)=1。 假设t=2t=2t=2时刻取到了3号盒，则q2=3q_2=3q2​=3，计算此条件下在观测序列为o3=ro_3=ro3​=r的后向概率β2(3)\\beta_2(3)β2​(3)为： β2(3)=P(o3=r,o2=b∣i2=q2,λ)=β3(1)a31P(o3=r∣i3=q1,λ)+β3(2)a32P(o3=r∣i3=q2,λ)+β3(3)a33P(o3=r∣i3=q3,λ)=15∗13+45∗13+25∗13=715\\begin{aligned} \\beta_2(3)&amp;=P(o_3=r,o_2=b|i_2=q_2,\\lambda)\\\\&amp;=\\beta _3(1)a_{31}P(o_3=r|i_3=q_1,\\lambda)+\\beta _3(2)a_{32}P(o_3=r|i_3=q_2,\\lambda)+\\beta _3(3)a_{33}P(o_3=r|i_3=q_3,\\lambda)\\\\&amp;=\\frac{1}{5}*\\frac{1}{3}+\\frac{4}{5}*\\frac{1}{3}+\\frac{2}{5}*\\frac{1}{3}=\\frac{7}{15} \\end{aligned}β2​(3)​=P(o3​=r,o2​=b∣i2​=q2​,λ)=β3​(1)a31​P(o3​=r∣i3​=q1​,λ)+β3​(2)a32​P(o3​=r∣i3​=q2​,λ)+β3​(3)a33​P(o3​=r∣i3​=q3​,λ)=51​∗31​+54​∗31​+52​∗31​=157​​ 同理可得β2(1)=715,β2(2)=715\\beta_2(1)=\\frac{7}{15},\\beta_2(2)=\\frac{7}{15}β2​(1)=157​,β2​(2)=157​ 同理计算出β1(1)=56225,β1(2)=56225,β1(3)=56225\\beta_1(1)=\\frac{56}{225},\\beta_1(2)=\\frac{56}{225},\\beta_1(3)=\\frac{56}{225}β1​(1)=22556​,β1​(2)=22556​,β1​(3)=22556​，带入公式，最后累加后得到P(O∣λ)=21135P(O|\\lambda)=\\frac{21}{135}P(O∣λ)=13521​。 综上，前向算法的时间复杂度为O(N2T)O(N^2T)O(N2T) 学习算法 在我们已知最终取到的小球观测序列为O=(r,b,r)O=(r,b,r)O=(r,b,r)的情况下，估计模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)的参数，使得在该模型下的观测序列概率P(O∣λ)P(O|\\lambda)P(O∣λ)最大。 监督学习和无监督学习方法的最大差别在于是否有观测序列对应的状态序列III。 监督学习方法 假设训练数据中包含SSS个长度相同的观测序列和对应的状态序列{(O1,I1),(O2,I2),...,(OS,IS)}\\{(O_1,I_1),(O_2,I_2),...,(O_S,I_S)\\}{(O1​,I1​),(O2​,I2​),...,(OS​,IS​)}，则可以使用极大似然估计法来估计隐马尔可夫模型的参数。 算法 估计转移概率aija_{ij}aij​ 假设样本中时刻 ttt 处于状态iii且时刻 t+1t+1t+1 处于状态 jjj 的频数为 AijA_{ij}Aij​ ，那么转移状态概率aija_{ij}aij​的估计是 a^ij=Aij∑j=1NAij,i=1,2,..N;j=1,2...,N\\hat{a} _{ij}=\\frac{A_{ij}}{\\sum_{j=1}^{N}A_{ij}} ,i=1,2,..N;j=1,2...,N a^ij​=∑j=1N​Aij​Aij​​,i=1,2,..N;j=1,2...,N 估计观测概率bj(k)b_j(k)bj​(k) 假设样本中状态为 jjj 并观测为 kkk 的频数是 BjkB_{jk}Bjk​ ，那么状态为 jjj 并观测为 kkk 的概率是 bj(k)b_j(k)bj​(k) 的估计是 b^j(k)=Bjk∑k=1MBjk,j=1,2,..M;k=1,2...,M\\hat{b} _{j}(k)=\\frac{B_{jk}}{\\sum_{k=1}^{M}B_{jk}} ,j=1,2,..M;k=1,2...,M b^j​(k)=∑k=1M​Bjk​Bjk​​,j=1,2,..M;k=1,2...,M 估计初始状态概率 πi\\pi_iπi​ πi^\\hat{\\pi_i}πi​^​ 为 SSS 个样本中初始状态为 qiq_iqi​ 的频率。 例子 （其实一开始我也没看懂上面说的是什么，不过还好现在懂了(๑•̀ㅂ•́)و✧，我真的很棒棒！） 这里依然使用小球举例子，我们重新拿了三个盒子，依然标为 A,B,CA,B,CA,B,C ，并且规定小y每次有放回地取4个球，并且只取6次，同时还要求小y在抽球的时候记录每次的观测序列 O=(o1,o2,o3,o4)O=(o_1,o_2,o_3,o_4)O=(o1​,o2​,o3​,o4​) 以及对应的状态序列 I=(i1,i2,i3,i4)I=(i_1,i_2,i_3,i_4)I=(i1​,i2​,i3​,i4​)。 小y抽完球之后，给我们发了一张这样的表： 状态序列 观测序列 I1=(A,B,C,B)I_1=(A,B,C,B)I1​=(A,B,C,B) O1=(r,b,r,b)O_1=(r,b,r,b)O1​=(r,b,r,b) I2=(A,A,B,A)I_2=(A,A,B,A)I2​=(A,A,B,A) O2=(r,r,b,b)O_2=(r,r,b,b)O2​=(r,r,b,b) I3=(C,A,B,C)I_3=(C,A,B,C)I3​=(C,A,B,C) O3=(r,b,b,r)O_3=(r,b,b,r)O3​=(r,b,b,r) I4=(A,C,A,B)I_4=(A,C,A,B)I4​=(A,C,A,B) O4=(b,r,b,b)O_4=(b,r,b,b)O4​=(b,r,b,b) I5=(C,A,C,B)I_5=(C,A,C,B)I5​=(C,A,C,B) O5=(r,b,b,r)O_5=(r,b,b,r)O5​=(r,b,b,r) I6=(B,C,C,A)I_6=(B,C,C,A)I6​=(B,C,C,A) O6=(b,b,r,r)O_6=(b,b,r,r)O6​=(b,b,r,r) 根据这张表，我们就可以利用最大似然估计法估计出这次使用的模型λ=(A,B,π)\\lambda=(A,B,\\pi)λ=(A,B,π)，方便起见我们使用编号 1,2,31,2,31,2,3 来代替盒子编号 A,B,CA,B,CA,B,C ，如 a^12\\hat a_{12}a^12​ 就表示从 AAA 盒转移到 BBB 号盒子的概率。 首先估计 a^11\\hat a_{11}a^11​ ，频数 A11A_{11}A11​ 为当前时刻取 AAA 盒且下一时刻依旧取 AAA 盒的次数，也即满足 a11a_{11}a11​ 条件的个数。上表中仅在 I2I_2I2​ 情况下符合a11a_{11}a11​ 条件，因此得到 A11=1A_{11}=1A11​=1 ，同理可得 A12=4,A13=2A_{12}=4,A_{13}=2A12​=4,A13​=2， 因此我们可以估计出a^11=11+4+2=17\\hat a_{11}=\\frac{1}{1+4+2}=\\frac{1}{7}a^11​=1+4+21​=71​ ，也可计算出a^12=47,a^13=27\\hat a_{12}=\\frac{4}{7},\\hat a_{13}=\\frac{2}{7}a^12​=74​,a^13​=72​。依次类推，我们可以得到A^\\hat AA^： A^=[17472714034472717]\\hat A=\\begin{bmatrix} \\frac{1}{7} &amp; \\frac{4}{7} &amp; \\frac{2}{7} \\\\ \\frac{1}{4} &amp; 0 &amp; \\frac{3}{4} \\\\ \\frac{4}{7} &amp;\\frac{2}{7} &amp;\\frac{1}{7} \\end{bmatrix} A^=⎣⎢⎡​71​41​74​​74​072​​72​43​71​​⎦⎥⎤​ 接着估计 b^1(r)\\hat b_1(r)b^1​(r) ，频数 B1rB_{1r}B1r​ 为取 AAA 盒的同时取到了红色球的次数，也即满足条件 b1(r)b_1(r)b1​(r) 的个数，在上表中容易得到 b1(r)=4b_1(r)=4b1​(r)=4 ，同理可得 b1(b)=5b_1(b)=5b1​(b)=5，因此得到 b^1(r)=44+5=49\\hat b_1(r)=\\frac{4}{4+5}=\\frac{4}{9}b^1​(r)=4+54​=94​。以此类推，我们可以得到 B^\\hat BB^： B^=[495917676828]\\hat B=\\begin{bmatrix} \\frac{4}{9} &amp; \\frac{5}{9} \\\\ \\frac{1}{7} &amp; \\frac{6}{7} \\\\ \\frac{6}{8} &amp;\\frac{2}{8} \\end{bmatrix} B^=⎣⎢⎡​94​71​86​​95​76​82​​⎦⎥⎤​ 最后估计 π^1\\hat \\pi_1π^1​ 即样本数据中初始状态为 AAA 盒的频率，显然π^1=36\\hat \\pi_1=\\frac{3}{6}π^1​=63​ ，同理得π^2=16,π^3=26\\hat \\pi_2=\\frac{1}{6},\\hat \\pi_3=\\frac{2}{6}π^2​=61​,π^3​=62​，因此 π^\\hat \\piπ^为： π^=(36,16,26)\\hat \\pi=(\\frac{3}{6},\\frac{1}{6},\\frac{2}{6}) π^=(63​,61​,62​) 无监督学习方法（EM算法） Baum-Welch算法，也叫做EM算法，和上面一种算法最大的不同就是，训练数据集中的长度为 TTT 状态序列 III 是未知的，只能通过观测序列 OOO 来学习模型 λ\\lambdaλ 的参数。那么求 P(O∣λ)P(O|\\lambda)P(O∣λ) 实际上是在计算一个含有隐变量的概率模型： P(O∣λ)=∑IP(O∣I,λ)P(I∣λ)P(O|\\lambda)=\\sum_I P(O|I,\\lambda)P(I|\\lambda) P(O∣λ)=I∑​P(O∣I,λ)P(I∣λ) 用小球模型解释这个问题，就是状态序列一栏全为Unknown 状态序列 观测序列 Unknown O1=(r,b,r,b)O_1=(r,b,r,b)O1​=(r,b,r,b) Unknown O2=(r,r,b,b)O_2=(r,r,b,b)O2​=(r,r,b,b) Unknown O3=(r,b,b,r)O_3=(r,b,b,r)O3​=(r,b,b,r) Unknown O4=(b,r,b,b)O_4=(b,r,b,b)O4​=(b,r,b,b) Unknown O5=(r,b,b,r)O_5=(r,b,b,r)O5​=(r,b,b,r) Unknown O6=(b,b,r,r)O_6=(b,b,r,r)O6​=(b,b,r,r) 算法 确定完全数据的对数似然函数 完全似然数据包括观测序列 OOO 和隐序列 III ，为(O,I)=(o1,o2,...,oT,i1,i2,...,iT)(O,I)=(o_1,o_2,...,o_T,i_1,i_2,...,i_T)(O,I)=(o1​,o2​,...,oT​,i1​,i2​,...,iT​) ，似然函数为 logP(O,I∣λ)logP(O,I|\\lambda)logP(O,I∣λ) 。 EM算法中的E步：求QQQ函数Q(λ,λˉ)Q(\\lambda,\\bar{\\lambda } )Q(λ,λˉ)","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"Supervise","slug":"Supervise","permalink":"http://example.com/tags/Supervise/"},{"name":"Mechine Learning","slug":"Mechine-Learning","permalink":"http://example.com/tags/Mechine-Learning/"}]},{"title":"N-gram Language Model","slug":"n-gram","date":"2021-11-17T07:59:52.000Z","updated":"2021-11-29T08:48:56.890Z","comments":true,"path":"2021/11/17/n-gram/","link":"","permalink":"http://example.com/2021/11/17/n-gram/","excerpt":"","text":"Introduce 在自然语言处理中，最常见的一个任务就是预测一个句子中的下一个单词，比如： Please turn your homework ... 在homework后可以连接很多单词，比如in、over，但不可能是接dog。为了更好的预测下一个单词，n-gram模型做的事情就是给可能出现在下一个位置的每个单词分配一个概率。对下一个单词出现概率的计算适用于很多场景，如语法纠错和输入法联想。 n-gram就是计算句子和单词序列概率最简单的模型，其中n指的是一个有n个单词序列的句子，如2-gram（‘I like’,‘I want’）或者3-gram（‘please turn on’,‘I like you’）等。 n-gram模型要做的就是根据给定的序列中前n-1个单词来估计第n个单词的概率，并且为整个句子分配一个概率。 假设有一个序列为：S=“我最喜欢自然语”，S的长度为7，如果我们想要计算下一个单词w=&quot;言&quot;的概率，那么数学表达式为P(w∣S)P(w|S)P(w∣S)，也可以写成P(言∣我最喜欢自然语)P(言|我最喜欢自然语)P(言∣我最喜欢自然语)。 通常我们会使用相对频率计数来估计这种概率，即在一个非常大语料库先统计S（“我最喜欢自然语”）出现的次数，接着统计S+w（“我最喜欢自然语言”）出现的次数，通过如下公式即可得出P(w∣S)P(w|S)P(w∣S)结果。 P(w∣S)=Count(S+w)Count(S)P(w|S)=\\frac{Count(S+w)}{Count(S)} P(w∣S)=Count(S)Count(S+w)​ 当然，我们也可以计算该序列的联合概率P(S)P(S)P(S)，即统计语料库中的S（“我最喜欢自然语”）在所有长度为7的句子中出现的次数。 P(S)=Count(S)Count(所有长度为7的句子)P(S)=\\frac{Count(S)}{Count(所有长度为7的句子)} P(S)=Count(所有长度为7的句子)Count(S)​ 如果语料库中有百万条句子，那么统计所有长度为7的句子数量会变得很麻烦，因此我们通常采用链式法则来估计P(S)P(S)P(S)。 Method 链式法则 概率的链式法则如下，其中X1:n−1=X1,X2,...,XnX_{1:n-1}=X_1,X_2,...,X_nX1:n−1​=X1​,X2​,...,Xn​ P(X1,...,Xn)=P(X1)P(X2∣X1)P(X3∣X1:2)...P(Xn∣X1:n−1)=∏k=1nP(Xk∣X1:k−1)\\begin{aligned} P(X_1,...,X_n)&amp;=P(X_1)P(X_2|X_1)P(X_3|X_{1:2})...P(X_n|X_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(X_k|X_{1:k-1}) \\end{aligned}P(X1​,...,Xn​)​=P(X1​)P(X2​∣X1​)P(X3​∣X1:2​)...P(Xn​∣X1:n−1​)=k=1∏n​P(Xk​∣X1:k−1​)​ 根据上述我们已知，假设存在长度为n的序列S，计算P(w1,w2,...,wn)P(w_1,w_2,...,w_n)P(w1​,w2​,...,wn​)，可以转化为如下算式 P(w1:n)=P(w1)P(w2∣w1)P(w3∣w1:2)...P(wn∣w1:n−1)=∏k=1nP(wk∣w1:k−1)\\begin{aligned} P(w_{1:n})&amp;=P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1})\\\\ &amp;=\\prod^n_{k=1}P(w_k|w_{1:k-1}) \\end{aligned}P(w1:n​)​=P(w1​)P(w2​∣w1​)P(w3​∣w1:2​)...P(wn​∣w1:n−1​)=k=1∏n​P(wk​∣w1:k−1​)​ 如果直接计算上式，我们需要获得n个不同的P(wk∣w1:k−1)P(w_k|w_{1:k-1})P(wk​∣w1:k−1​)，计算起来也很麻烦。因此n-gram模型为了减轻计算量，假设只用少量的已知词来估计下一个词，比如2-gram(bigram)，来解决上述问题。 bigram 假设想要在已知前n-1个次的请况下估计第n个词出现的概率，即计算P(wn∣w1:n−1)P(w_n|w_{1:n-1})P(wn​∣w1:n−1​)。bigram模型通常的做法是，用P(wn∣wn−1)P(w_n|w_{n-1})P(wn​∣wn−1​)代替计算P(wn∣w1:n−1)P(w_n|w_{1:n-1})P(wn​∣w1:n−1​)。例如上述例子，计算P(言∣我最喜欢自然语)P(言|我最喜欢自然语)P(言∣我最喜欢自然语)可以通过计算P(言∣语)P(言|语)P(言∣语)来近似代替。 因此，当我们使用bigram模型来预测下一个单词的概率是，我们可以使用如下近似估算 P(wn∣w1:n−1)≈P(wn∣wn−1)P(w_n|w_{1:n-1})\\approx P(w_n|w_{n-1}) P(wn​∣w1:n−1​)≈P(wn​∣wn−1​) 实现上述假设，依赖于马尔科夫链（Markov）。 马尔科夫链 Reference N-gram Language Model","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"Model","slug":"Model","permalink":"http://example.com/tags/Model/"}]},{"title":"NNML论文阅读","slug":"NNML论文阅读","date":"2021-11-17T07:42:48.000Z","updated":"2021-11-30T04:43:04.974Z","comments":true,"path":"2021/11/17/NNML论文阅读/","link":"","permalink":"http://example.com/2021/11/17/NNML%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","excerpt":"","text":"本文是2003年发表在Journal of Machine Learning Research上的文章，名为A Neural Probabilistic Language Model。或许现在受到的关注远远不如在此之前的n-gram和在此之后的word2vec要多，主要原因在于文中提出的训练方式代价极大。 导读 统计语言建模的主要目标是学习一种语言中单词序列的联合概率函数，但由于维度诅咒（curse of dimensionality）使得目标很难实现。维度诅咒在该类任务主要表现为：测试集中单词序列可能和训练集中的序列不一致，导致模型的泛化能力变得很差。 传统的n-gram方法通过连接短重叠序列来获得泛化，效果虽然很好，但随着词表和n的增大容易使得n-gram参数爆炸性增多。比如对词汇量 ∣V∣=100,000|V|=100,000∣V∣=100,000 的单词进行10-gram建模时，产生的10-gram的参数达到 ∣V∣n=10000010|V|^n=100000^{10}∣V∣n=10000010 之多。因此对n-gram而言，对较长序列的建模非常困难。 最重要的是，n-gram没有考虑单词之间的相似性，其本质还是one-hot向量。比如它无法归纳出“The cat is walking in the bedroom”和“A dog was running in a room”之间的相似性。 NNML 解决方法 基于上述问题，NNML首先提出使用分布式解决维度诅咒问题，具体方法如下 将词表中的每一个词和一个分布式单词特征向量（word feature vector）关联。 用序列中单词的特征向量来表示序列的联合概率密度。 同时学习单词特征向量和概率函数的参数。 特征向量的每一个维度都代表了单词的不同方面，而且每个单词都与向量空间中的一个点相关联，并且特征向量的数量远小于词汇的数量。 简单举例：假设一个词表有∣V∣=20000|V|=20000∣V∣=20000 个词，单词are排在第1位，过去的方式会将are表示成一个长度为20000的one-hot向量 [1,0,0,...,0][1,0,0,...,0][1,0,0,...,0] ，但在NNML中，只需要将其表示为长度 m=30,50m=30,50m=30,50 或 $ 100 $ 的特征向量即可，这样就极大缩小了参数量，从原本的 ∣V∣2|V|^2∣V∣2 缩小到 ∣V∣m|V|^m∣V∣m。 因为“相似”的词应该有一个相似的特征向量，而且由于概率密度函数是这些特征值的平滑函数，所以特征值的微小变化可以让概率发生微小变化，可以增加泛化效果。 模型架构 假设现有一个词表 VVV ，训练集是一串形如 w1,w2,...,wTw_1,w_2,...,w_Tw1​,w2​,...,wT​ 的单词序列，其中 wt∈Vw_t \\in Vwt​∈V ，模型的目标是训练一个模型fff ，使得 f=(wt,...,wt−n+1)=P^(wt∣w1t−1)f=(w_t,...,w_{t-n+1})=\\hat P(w_t|w_1^{t-1})f=(wt​,...,wt−n+1​)=P^(wt​∣w1t−1​) ，并且保证对于任意的w1t−1w_1^{t-1}w1t−1​，均有 ∑i=1∣V∣f(i,wt−1,...,wt−n+1)\\sum _{i=1}^{|V|}f(i,w_{t-1},...,w_{t-n+1})∑i=1∣V∣​f(i,wt−1​,...,wt−n+1​) 且 f&gt;0f&gt;0f&gt;0。 上面的约束条件很绕口，简单解释就是无论对任意的n-1个序列，∑i=第1个单词i=第V个单词P^(i∣任意n−1序列)=1\\sum_{i=第1个单词}^{i=第V个单词} \\hat P(i|任意n-1序列)=1∑i=第1个单词i=第V个单词​P^(i∣任意n−1序列)=1。 NNML将从以下两部分构建这个模型： 建立词表 VVV 中的每一个单词 iii 到实向量 $C(i)\\in \\mathbb{R}^m $ 的映射，最后得到的CCC 是一个∣V∣×m|V|\\times m∣V∣×m 大小的矩阵。 用 CCC 表示单词的概率函数：定义一个函数 ggg ，ggg 可以将输入的上下文中的单词特征向量序列（C(wt−n+1),...,C(wt−1)C(w_t-n+1),...,C(w_{t-1})C(wt​−n+1),...,C(wt−1​)）映射成下一个单词 wtw_twt​ 的条件概率分布。ggg 的输出是一个向量，该向量的第 iii 个元素是对概率 P^(wt∣w1t−1)\\hat P(w_t|w_1^{t-1})P^(wt​∣w1t−1​) 的估计。因此可以得到一个公式： f=(i,wt,...,wt−n+1)=g(i,C(wt−n+1),...,C(wt−1))f=(i,w_t,...,w_{t-n+1})=g(i,C(w_t-n+1),...,C(w_{t-1})) f=(i,wt​,...,wt−n+1​)=g(i,C(wt​−n+1),...,C(wt−1​)) 函数 fff 是这两个映射（CCC 和 ggg）的组合，并且上下文中所有单词都共享同一个 CCC ，CCC 的参数只是词向量本身，CCC 可以表示成一个大小为·∣V∣×m|V|\\times m∣V∣×m 的矩阵，它的第 iii 行就是单词 iii 的特征向量 C(i)C(i)C(i) 值。 函数 ggg 可以通过前馈或递归神经网络或另一个参数化函数来实现，其中参数为ωωω 。整个参数集为 θ=(C,ω)\\theta=(C,\\omega)θ=(C,ω) ，优化模型的方式为最大化对数似然函数 LLL： L=1T∑tlogf(wt,wt−1,wt−n+1,θ)+R(θ)L=\\frac{1}{T}\\sum_t log f(w_t,w_{t-1},w_{t-n+1},\\theta)+R(\\theta) L=T1​t∑​logf(wt​,wt−1​,wt−n+1​,θ)+R(θ) 其中 R(θ)R(θ)R(θ) 是正则化项,，且 RRR 的权值衰减惩罚只适用于神经网络和 CCC 矩阵的权值，而不适用于偏差 最终模型的结构图如下: 可以看出NNML一共有两个隐藏层，一个是线性的共享词特征层 CCC，一个是普通的 tanhtanhtanh 隐藏层，最后输出层是一个 softmaxsoftmaxsoftmax 层用来保证正概率和始终为1。 P^(wt∣wt−1,...,wt−n+1)=eywt∑ieyi\\hat P(w_t|w_{t-1},...,w_{t-n+1})=\\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}} P^(wt​∣wt−1​,...,wt−n+1​)=∑i​eyi​eywt​​​ 这里的yiy_iyi​ 是每个输出的单词 iii 的非标准化对数概率，计算方式如下： y=b+Wx+Utanh(d+Hx)y=b+Wx+Utanh(d+Hx) y=b+Wx+Utanh(d+Hx) 其中 WWW 是 xxx 的参数，xxx 是词特征激活向量，由一串输入序列的词特征向量拼接组成。 x=(C(wt−1),C(wt−2),c(wt−n+1))x = (C (w_{t−1}),C (w_{t−2}),c (w_{t-n+1})) x=(C(wt−1​),C(wt−2​),c(wt−n+1​)) 假设 hhh 是隐藏单元的个数，mmm 是每个单词的特征向量维度，虚线表示的是单词的特征向量和输出向量的直连，如果不需要的话，此处的参数 WWW 可以置为0，模型的自由参数如下：输出偏置 bbb （∣V∣|V|∣V∣ 个元素）、隐藏层偏置 ddd （hhh 个元素）、隐藏层权重 UUU （一个大小为∣V∣×h|V| \\times h∣V∣×h 的矩阵）、单词特征到输出层的权重 WWW（一个大小为 ∣V∣×(n−1)m|V| \\times (n-1)m∣V∣×(n−1)m 的矩阵）、隐藏层权重 HHH （一个大小为 h×(n−1)mh \\times (n-1)mh×(n−1)m 的矩阵）和词特征矩阵 CCC （一个大小为 ∣V∣×m|V| \\times m∣V∣×m 的矩阵）。 因此参数集合 θ\\thetaθ 可表示为如下： θ=(b,d,W,U,H,C)\\theta=(b,d,W,U,H,C) θ=(b,d,W,U,H,C) 自由参数的总数为 ∣V∣(1+nm+h)+h(1+(n−1)m)|V|(1+nm+h)+h(1+(n-1)m)∣V∣(1+nm+h)+h(1+(n−1)m)，因为 hhh 的值一般很小，而 ∣V∣|V|∣V∣ 的值可以取很大，所以自由参数的的主导部分是 ∣V∣(nm+h)|V|(nm+h)∣V∣(nm+h)，因为是最大化似然函数，因此最后用随机梯度上升法更新参数 θ\\thetaθ 。 扩展 个人觉得这篇论文比较厉害的地方就在于自己对存在的计算量问题提出了两个解法，并影响到后来的word2vec模型。NNML训练时需要的代价很大，为了减少这些代价，作者提供了两种方法： 例如使用单词的聚类将网络分解成无数个子网，从而通过训练小网络缩短训练时间。 用树形结构表示条件概率，在每个节点上应用神经网络。每个非叶子节点表示给定上下文预测的单词属于某一单词类别（word class）的概率，叶子表示给定上下文预测出某一单词（word）的概率。这种类型的表示有可能将计算时间缩短到 ∣V∣/log∣V|V|/log|V∣V∣/log∣V。 总结 在NNML模型中，由于输入窗口的大小和词汇表的大小线性，因此模型的参数量可以有效地缩放，但获得输出概率所需的计算量远远大于n-gram模型所需的计算量。主要原因是，在n-gram模型中，获得一个特定的P(wt∣wt−1，…，wt−n+1)P(w_t|w_{t−1}，…，w_{t−n+1})P(wt​∣wt−1​，…，wt−n+1​)不需要计算词汇表中所有单词的概率。 因此NNML虽然解决了长序列以及泛化问题，但是神经网络模型需要的过大的训练代价和时间依旧是需要解决的问题。","categories":[{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/tags/Paper/"}]},{"title":"nltk使用指南","slug":"nltk使用指南","date":"2021-11-16T12:28:53.000Z","updated":"2021-11-29T08:48:47.586Z","comments":true,"path":"2021/11/16/nltk使用指南/","link":"","permalink":"http://example.com/2021/11/16/nltk%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"","text":"安装方式 先使用pip安装pip install nltk，接着在命令行中输入如下句子，即可下载。 12import nltknltk.download() 如果报错，则修改Sever Index为http://www.nltk.org/nltk_data/即可下载任意数据。但也可能存在即使改了也不生效的情况，可以上网查找一波前辈们已经整理好的文件夹，按照他们的博文里面的方式解压存放。 这里注意解压之后把package改名为nltk_data后移动到相应的位置即可。 使用方法 查找指定单词上下文，使用函数concorance 1text1.concordance(&#x27;monstrous&#x27;) 查找文本中意思相近的词语，使用函数similar 1text1.similar(&#x27;monstrous&#x27;) 查找两个及以上词汇共用的上下文，使用函数common_context 1text2.common_contexts([&#x27;monstrous&#x27;,&#x27;very&#x27;]) 根据某一风格生成随机文本，使用函数generate 1text2.generate() 查看文本中某一词汇出现的次数 1text3.count(&#x27;smote&#x27;) 可以通过如下函数实现文本词汇丰富度测量 12def lexical_diversity(text): return len(text)/len(set(text)) 查看单个词汇在全文中占比 123def percentage(count,total): return 100*count/totalpercentage(text4.count(&#x27;a&#x27;),len(text4)) 查找给定单词在文本中的频率分布，使用FreqDist，该函数的返回值为一个字典。 12345fdist1=FreqDist(text1)fdist1#outputFreqDist(&#123;&#x27;,&#x27;: 18713, &#x27;the&#x27;: 13721, &#x27;.&#x27;: 6862, &#x27;of&#x27;: 6536, &#x27;and&#x27;: 6024, &#x27;a&#x27;: 4569, &#x27;to&#x27;: 4542, &#x27;;&#x27;: 4072, &#x27;in&#x27;: 3916, &#x27;that&#x27;: 2982, ...&#125;) 也可以绘制图text1文本最常见的50个词 接着可以查看文本中任意词汇出现的频率 1fdist1.freq(&#x27;very&#x27;) 通过bigrams可以实现词语两两搭配 1234[i for i in bigrams([&#x27;I&#x27;,&#x27;love&#x27;,&#x27;nlp&#x27;])]#output[(&#x27;I&#x27;, &#x27;love&#x27;), (&#x27;love&#x27;, &#x27;nlp&#x27;)] 查找词汇搭配使用collocations() 1text4.collocations()","categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"NLTK","slug":"NLTK","permalink":"http://example.com/tags/NLTK/"}]},{"title":"旅游计划","slug":"迪士尼","date":"2021-11-16T08:41:06.000Z","updated":"2021-11-28T09:25:51.313Z","comments":true,"path":"2021/11/16/迪士尼/","link":"","permalink":"http://example.com/2021/11/16/%E8%BF%AA%E5%A3%AB%E5%B0%BC/","excerpt":"","text":"期待迪士尼之行","categories":[{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}],"categories":[{"name":"Learn","slug":"Learn","permalink":"http://example.com/categories/Learn/"},{"name":"Study","slug":"Study","permalink":"http://example.com/categories/Study/"},{"name":"travel","slug":"travel","permalink":"http://example.com/categories/travel/"},{"name":"shanghai","slug":"travel/shanghai","permalink":"http://example.com/categories/travel/shanghai/"}],"tags":[{"name":"Code","slug":"Code","permalink":"http://example.com/tags/Code/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/tags/Paper/"},{"name":"Supervise","slug":"Supervise","permalink":"http://example.com/tags/Supervise/"},{"name":"Mechine Learning","slug":"Mechine-Learning","permalink":"http://example.com/tags/Mechine-Learning/"},{"name":"Model","slug":"Model","permalink":"http://example.com/tags/Model/"},{"name":"NLTK","slug":"NLTK","permalink":"http://example.com/tags/NLTK/"},{"name":"travel","slug":"travel","permalink":"http://example.com/tags/travel/"}]}